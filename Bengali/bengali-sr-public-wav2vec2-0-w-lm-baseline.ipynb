{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":52324,"databundleVersionId":6229904,"sourceType":"competition"},{"sourceId":4143520,"sourceType":"datasetVersion","datasetId":2447262},{"sourceId":140325995,"sourceType":"kernelVersion"}],"dockerImageVersionId":30528,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About\n\nAfter seeing [this post](https://www.kaggle.com/competitions/bengaliai-speech/discussion/425496)„ÄÅI've been curious what the Public LB Score the combination of public trained models would get.\n\nIn this notebook I used two public models from hugging faces:\n* `https://huggingface.co/ai4bharat/indicwav2vec_v1_bengali` for Wav2vec2CTC Model only\n* `https://huggingface.co/arijitx/wav2vec2-xls-r-300m-bengali` for Language Model\n\nI didn't trained these models using the competitaion data at all. I just want to know public models score as baseline.  \n\nSo we may get higher and higher score by fine-tuning on competition data.","metadata":{}},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"!cp -r ../input/python-packages2 ./\n\n!tar xvfz ./python-packages2/jiwer.tgz\n!pip install ./jiwer/jiwer-2.3.0-py3-none-any.whl -f ./ --no-index\n!tar xvfz ./python-packages2/normalizer.tgz\n!pip install ./normalizer/bnunicodenormalizer-0.0.24.tar.gz -f ./ --no-index\n!tar xvfz ./python-packages2/pyctcdecode.tgz\n!pip install ./pyctcdecode/attrs-22.1.0-py2.py3-none-any.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/exceptiongroup-1.0.0rc9-py3-none-any.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/hypothesis-6.54.4-py3-none-any.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/pygtrie-2.5.0.tar.gz -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/sortedcontainers-2.4.0-py2.py3-none-any.whl -f ./ --no-index --no-deps\n!pip install ./pyctcdecode/pyctcdecode-0.4.0-py2.py3-none-any.whl -f ./ --no-index --no-deps\n\n!tar xvfz ./python-packages2/pypikenlm.tgz\n!pip install ./pypikenlm/pypi-kenlm-0.1.20220713.tar.gz -f ./ --no-index --no-deps","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-23T19:42:39.617840Z","iopub.execute_input":"2024-07-23T19:42:39.618116Z","iopub.status.idle":"2024-07-23T19:43:52.031956Z","shell.execute_reply.started":"2024-07-23T19:42:39.618091Z","shell.execute_reply":"2024-07-23T19:43:52.030736Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"jiwer/\njiwer/jiwer-2.3.0-py3-none-any.whl\njiwer/python-Levenshtein-0.12.2.tar.gz\njiwer/setuptools-65.3.0-py3-none-any.whl\nLooking in links: ./\nProcessing ./jiwer/jiwer-2.3.0-py3-none-any.whl\nINFO: pip is looking at multiple versions of jiwer to determine which version is compatible with other requirements. This could take a while.\n\u001b[31mERROR: Could not find a version that satisfies the requirement python-Levenshtein==0.12.2 (from jiwer) (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for python-Levenshtein==0.12.2\u001b[0m\u001b[31m\n\u001b[0mnormalizer/\nnormalizer/bnunicodenormalizer-0.0.24.tar.gz\nLooking in links: ./\nProcessing ./normalizer/bnunicodenormalizer-0.0.24.tar.gz\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: bnunicodenormalizer\n  Building wheel for bnunicodenormalizer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bnunicodenormalizer: filename=bnunicodenormalizer-0.0.24-py3-none-any.whl size=17628 sha256=6c3f2799150014ca158e556d1ec2d1ce886d49590b76bfe928cea79533f38f21\n  Stored in directory: /root/.cache/pip/wheels/78/d7/75/6986dc3616718f950b80e3bd79a796ef618eaef6cd800e7909\nSuccessfully built bnunicodenormalizer\nInstalling collected packages: bnunicodenormalizer\nSuccessfully installed bnunicodenormalizer-0.0.24\npyctcdecode/\npyctcdecode/hypothesis-6.54.4-py3-none-any.whl\npyctcdecode/sortedcontainers-2.4.0-py2.py3-none-any.whl\npyctcdecode/exceptiongroup-1.0.0rc9-py3-none-any.whl\npyctcdecode/pyctcdecode-0.4.0-py2.py3-none-any.whl\npyctcdecode/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\npyctcdecode/attrs-22.1.0-py2.py3-none-any.whl\npyctcdecode/pygtrie-2.5.0.tar.gz\nLooking in links: ./\nProcessing ./pyctcdecode/attrs-22.1.0-py2.py3-none-any.whl\nInstalling collected packages: attrs\n  Attempting uninstall: attrs\n    Found existing installation: attrs 23.1.0\n    Uninstalling attrs-23.1.0:\n      Successfully uninstalled attrs-23.1.0\nSuccessfully installed attrs-22.1.0\nLooking in links: ./\nProcessing ./pyctcdecode/exceptiongroup-1.0.0rc9-py3-none-any.whl\nInstalling collected packages: exceptiongroup\n  Attempting uninstall: exceptiongroup\n    Found existing installation: exceptiongroup 1.1.1\n    Uninstalling exceptiongroup-1.1.1:\n      Successfully uninstalled exceptiongroup-1.1.1\nSuccessfully installed exceptiongroup-1.0.0rc9\nLooking in links: ./\nProcessing ./pyctcdecode/hypothesis-6.54.4-py3-none-any.whl\nInstalling collected packages: hypothesis\nSuccessfully installed hypothesis-6.54.4\nLooking in links: ./\n\u001b[31mERROR: numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n\u001b[0mLooking in links: ./\nProcessing ./pyctcdecode/pygtrie-2.5.0.tar.gz\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: pygtrie\n  Building wheel for pygtrie (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pygtrie: filename=pygtrie-2.5.0-py3-none-any.whl size=20945 sha256=6b5802997e8770d9de39c73e53d2601b9364348a284d9fdc00d24eccb94e0faa\n  Stored in directory: /root/.cache/pip/wheels/78/28/09/b62c97a3e77102645c7ecc78c97580ad57090b1eee5438d6ac\nSuccessfully built pygtrie\nInstalling collected packages: pygtrie\nSuccessfully installed pygtrie-2.5.0\nLooking in links: ./\nProcessing ./pyctcdecode/sortedcontainers-2.4.0-py2.py3-none-any.whl\nsortedcontainers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nLooking in links: ./\nProcessing ./pyctcdecode/pyctcdecode-0.4.0-py2.py3-none-any.whl\nInstalling collected packages: pyctcdecode\nSuccessfully installed pyctcdecode-0.4.0\npypikenlm/\npypikenlm/pypi-kenlm-0.1.20220713.tar.gz\nLooking in links: ./\nProcessing ./pypikenlm/pypi-kenlm-0.1.20220713.tar.gz\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: pypi-kenlm\n  Building wheel for pypi-kenlm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypi-kenlm: filename=pypi_kenlm-0.1.20220713-cp310-cp310-linux_x86_64.whl size=333264 sha256=e7abb3e3e5859353e8bfc68144f29c6eb6f15676ba4bad5d5cbaa9ebcc9f8480\n  Stored in directory: /root/.cache/pip/wheels/1e/7a/db/27645fac296d5d5ba5c461b1af834eebc0ba4643290dbc5476\nSuccessfully built pypi-kenlm\nInstalling collected packages: pypi-kenlm\nSuccessfully installed pypi-kenlm-0.1.20220713\n","output_type":"stream"}]},{"cell_type":"code","source":"rm -r python-packages2 jiwer normalizer pyctcdecode pypikenlm","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:43:53.076302Z","iopub.execute_input":"2024-07-23T19:43:53.076773Z","iopub.status.idle":"2024-07-23T19:43:54.105927Z","shell.execute_reply.started":"2024-07-23T19:43:53.076729Z","shell.execute_reply":"2024-07-23T19:43:54.104778Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"rm: cannot remove 'python-packages2': No such file or directory\nrm: cannot remove 'jiwer': No such file or directory\nrm: cannot remove 'normalizer': No such file or directory\nrm: cannot remove 'pyctcdecode': No such file or directory\nrm: cannot remove 'pypikenlm': No such file or directory\n","output_type":"stream"}]},{"cell_type":"code","source":"import typing as tp\nfrom pathlib import Path\nfrom functools import partial\nfrom dataclasses import dataclass, field\n\nimport pandas as pd\nimport pyctcdecode\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nimport librosa\n\nimport pyctcdecode\nimport kenlm\nimport torch\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ProcessorWithLM, Wav2Vec2ForCTC\nfrom bnunicodenormalizer import Normalizer\n\nimport cloudpickle as cpkl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-23T19:44:16.073186Z","iopub.execute_input":"2024-07-23T19:44:16.073763Z","iopub.status.idle":"2024-07-23T19:44:16.079245Z","shell.execute_reply.started":"2024-07-23T19:44:16.073736Z","shell.execute_reply":"2024-07-23T19:44:16.078313Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ROOT = Path.cwd().parent\nINPUT = ROOT / \"input\"\nDATA = INPUT / \"bengaliai-speech\"\nTRAIN = DATA / \"train_mp3s\"\nTEST = DATA / \"test_mp3s\"\n\nSAMPLING_RATE = 16_000\nMODEL_PATH = INPUT / \"bengali-sr-download-public-trained-models/indicwav2vec_v1_bengali/\"\nLM_PATH = INPUT / \"bengali-sr-download-public-trained-models/wav2vec2-xls-r-300m-bengali/language_model/\"","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:44:25.073273Z","iopub.execute_input":"2024-07-23T19:44:25.073653Z","iopub.status.idle":"2024-07-23T19:44:25.078906Z","shell.execute_reply.started":"2024-07-23T19:44:25.073614Z","shell.execute_reply":"2024-07-23T19:44:25.078023Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### load model, processor, decoder","metadata":{}},{"cell_type":"code","source":"model = Wav2Vec2ForCTC.from_pretrained(MODEL_PATH)\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:46:17.561675Z","iopub.execute_input":"2024-07-23T19:46:17.562740Z","iopub.status.idle":"2024-07-23T19:46:20.642240Z","shell.execute_reply.started":"2024-07-23T19:46:17.562691Z","shell.execute_reply":"2024-07-23T19:46:20.641444Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"vocab_dict = processor.tokenizer.get_vocab()\nsorted_vocab_dict = {k: v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n\ndecoder = pyctcdecode.build_ctcdecoder(\n    list(sorted_vocab_dict.keys()),\n    str(LM_PATH / \"5gram.bin\"),\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:47:02.527772Z","iopub.execute_input":"2024-07-23T19:47:02.528629Z","iopub.status.idle":"2024-07-23T19:47:02.676040Z","shell.execute_reply.started":"2024-07-23T19:47:02.528588Z","shell.execute_reply":"2024-07-23T19:47:02.675009Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"processor_with_lm = Wav2Vec2ProcessorWithLM(\n    feature_extractor=processor.feature_extractor,\n    tokenizer=processor.tokenizer,\n    decoder=decoder\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:48:23.559675Z","iopub.execute_input":"2024-07-23T19:48:23.560349Z","iopub.status.idle":"2024-07-23T19:48:23.564880Z","shell.execute_reply.started":"2024-07-23T19:48:23.560315Z","shell.execute_reply":"2024-07-23T19:48:23.563934Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## prepare dataloader","metadata":{}},{"cell_type":"code","source":"class BengaliSRTestDataset(torch.utils.data.Dataset):\n    \n    def __init__(\n        self,\n        audio_paths: list[str],\n        sampling_rate: int\n    ):\n        self.audio_paths = audio_paths\n        self.sampling_rate = sampling_rate\n        \n    def __len__(self,):\n        return len(self.audio_paths)\n    \n    def __getitem__(self, index: int):\n        audio_path = self.audio_paths[index]\n        sr = self.sampling_rate\n        w = librosa.load(audio_path, sr=sr, mono=False)[0]\n        \n        return w","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:48:30.903458Z","iopub.execute_input":"2024-07-23T19:48:30.903783Z","iopub.status.idle":"2024-07-23T19:48:30.910194Z","shell.execute_reply.started":"2024-07-23T19:48:30.903756Z","shell.execute_reply":"2024-07-23T19:48:30.909284Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(DATA / \"sample_submission.csv\", dtype={\"id\": str})\nprint(test.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:48:36.792149Z","iopub.execute_input":"2024-07-23T19:48:36.792427Z","iopub.status.idle":"2024-07-23T19:48:36.801887Z","shell.execute_reply.started":"2024-07-23T19:48:36.792403Z","shell.execute_reply":"2024-07-23T19:48:36.800973Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"             id                                           sentence\n0  0f3dac00655e  ‡¶è‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶ì ‡¶®‡¶ø‡¶â‡¶ú‡¶ø‡¶≤‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶° ‡¶è ‡¶ï‡ßç‡¶∞‡¶ø‡¶ï‡ßá‡¶ü ‡¶¶‡¶≤‡ßá‡¶∞ ‡¶π‡¶Ø‡¶º‡ßá‡¶ì ‡¶ñ‡ßá‡¶≤‡¶õ...\n1  a9395e01ad21  ‡¶è‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶ì ‡¶®‡¶ø‡¶â‡¶ú‡¶ø‡¶≤‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶° ‡¶è ‡¶ï‡ßç‡¶∞‡¶ø‡¶ï‡ßá‡¶ü ‡¶¶‡¶≤‡ßá‡¶∞ ‡¶π‡¶Ø‡¶º‡ßá‡¶ì ‡¶ñ‡ßá‡¶≤‡¶õ...\n2  bf36ea8b718d  ‡¶è‡¶õ‡¶æ‡¶°‡¶º‡¶æ‡¶ì ‡¶®‡¶ø‡¶â‡¶ú‡¶ø‡¶≤‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶° ‡¶è ‡¶ï‡ßç‡¶∞‡¶ø‡¶ï‡ßá‡¶ü ‡¶¶‡¶≤‡ßá‡¶∞ ‡¶π‡¶Ø‡¶º‡ßá‡¶ì ‡¶ñ‡ßá‡¶≤‡¶õ...\n","output_type":"stream"}]},{"cell_type":"code","source":"test_audio_paths = [str(TEST / f\"{aid}.mp3\") for aid in test[\"id\"].values]","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:48:44.720114Z","iopub.execute_input":"2024-07-23T19:48:44.721090Z","iopub.status.idle":"2024-07-23T19:48:44.726316Z","shell.execute_reply.started":"2024-07-23T19:48:44.721046Z","shell.execute_reply":"2024-07-23T19:48:44.725211Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"test_dataset = BengaliSRTestDataset(\n    test_audio_paths, SAMPLING_RATE\n)\n\ncollate_func = partial(\n    processor_with_lm.feature_extractor,\n    return_tensors=\"pt\", sampling_rate=SAMPLING_RATE,\n    padding=True,\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=8, shuffle=False,\n    num_workers=2, collate_fn=collate_func, drop_last=False,\n    pin_memory=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:49:02.398656Z","iopub.execute_input":"2024-07-23T19:49:02.398951Z","iopub.status.idle":"2024-07-23T19:49:02.406738Z","shell.execute_reply.started":"2024-07-23T19:49:02.398927Z","shell.execute_reply":"2024-07-23T19:49:02.405970Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"if not torch.cuda.is_available():\n    device = torch.device(\"cpu\")\nelse:\n    device = torch.device(\"cuda\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:49:07.090007Z","iopub.execute_input":"2024-07-23T19:49:07.090326Z","iopub.status.idle":"2024-07-23T19:49:07.095867Z","shell.execute_reply.started":"2024-07-23T19:49:07.090298Z","shell.execute_reply":"2024-07-23T19:49:07.094820Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"model = model.to(device)\nmodel = model.eval()\nmodel = model.half()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:49:13.403089Z","iopub.execute_input":"2024-07-23T19:49:13.403882Z","iopub.status.idle":"2024-07-23T19:49:13.421982Z","shell.execute_reply.started":"2024-07-23T19:49:13.403851Z","shell.execute_reply":"2024-07-23T19:49:13.421230Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"pred_sentence_list = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        x = batch[\"input_values\"]\n        x = x.to(device, non_blocking=True)\n        with torch.cuda.amp.autocast(True):\n            y = model(x).logits\n        y = y.detach().cpu().numpy()\n        \n        for l in y:  \n            sentence = processor_with_lm.decode(l, beam_width=512).text\n            pred_sentence_list.append(sentence)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:49:29.358710Z","iopub.execute_input":"2024-07-23T19:49:29.359067Z","iopub.status.idle":"2024-07-23T19:49:31.978668Z","shell.execute_reply.started":"2024-07-23T19:49:29.359031Z","shell.execute_reply":"2024-07-23T19:49:31.977606Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"253d95b07d1d4d68acce652b73ca69c1"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Make Submission","metadata":{}},{"cell_type":"code","source":"bnorm = Normalizer()\n\ndef postprocess(sentence):\n    period_set = set([\".\", \"?\", \"!\", \"‡•§\"])\n    _words = [bnorm(word)['normalized']  for word in sentence.split()]\n    sentence = \" \".join([word for word in _words if word is not None])\n    try:\n        if sentence[-1] not in period_set:\n            sentence+=\"‡•§\"\n    except:\n        # print(sentence)\n        sentence = \"‡•§\"\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:49:41.716932Z","iopub.execute_input":"2024-07-23T19:49:41.717713Z","iopub.status.idle":"2024-07-23T19:49:41.725893Z","shell.execute_reply.started":"2024-07-23T19:49:41.717670Z","shell.execute_reply":"2024-07-23T19:49:41.724785Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"pp_pred_sentence_list = [\n    postprocess(s) for s in tqdm(pred_sentence_list)]","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:49:46.282932Z","iopub.execute_input":"2024-07-23T19:49:46.283854Z","iopub.status.idle":"2024-07-23T19:49:46.317052Z","shell.execute_reply.started":"2024-07-23T19:49:46.283815Z","shell.execute_reply":"2024-07-23T19:49:46.316145Z"},"trusted":true},"execution_count":77,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c4ced76f07f40468d4a7dfb3aa3e2c0"}},"metadata":{}}]},{"cell_type":"code","source":"test[\"sentence\"] = pp_pred_sentence_list\n\ntest.to_csv(\"submission.csv\", index=False)\n\nprint(test.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-23T19:49:51.111592Z","iopub.execute_input":"2024-07-23T19:49:51.112446Z","iopub.status.idle":"2024-07-23T19:49:51.124314Z","shell.execute_reply.started":"2024-07-23T19:49:51.112411Z","shell.execute_reply":"2024-07-23T19:49:51.123316Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"             id                                           sentence\n0  0f3dac00655e                          ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶¨‡ßü‡¶∏ ‡¶π‡¶≤‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶ø‡¶¶‡ßá‡¶∂‡¶ø‡•§\n1  a9395e01ad21  ‡¶ï‡ßÄ ‡¶ï‡¶æ‡¶∞‡¶£‡ßá ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶§‡¶æ‡¶¨‡ßé ‡¶ï‡¶æ‡¶≤ ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶è ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶≤ ‡¶¶‡ßà‡¶¨ ‡¶¶‡ßÅ...\n2  bf36ea8b718d  ‡¶è ‡¶ï‡¶æ‡¶∞‡¶£‡ßá ‡¶∏‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶ø‡¶§ ‡¶π‡¶æ‡¶∞‡ßá ‡¶™‡¶∞‡¶ø‡¶¨‡¶π‡¶®‡¶ú‡¶®‡¶ø‡¶§ ‡¶ï‡ßç‡¶∑‡¶§‡¶ø ...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## EOF","metadata":{}}]}