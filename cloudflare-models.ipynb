{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudflare models\n",
    "\n",
    "This notebooks shows how to retrieve the current available models from Cloudflare's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# load environment variables\n",
    "import os \n",
    "env_path=\"/Users/philipp/Projects/huggingface/model-recommender/.env\"\n",
    "os.environ = {**os.environ, **{env.split(\"=\")[0]: env.split(\"=\")[1] for env in open(env_path, \"r\").readlines()}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 43 models available\n",
      "There are 12 Hugging Face models available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'b375b9cb-51e5-40d7-a601-45d86387cb8c',\n",
       " 'source': 2,\n",
       " 'name': '@hf/thebloke/orca-2-13b-awq',\n",
       " 'description': 'Orca 2 is a helpful assistant that is built for research purposes only and provides a single turn response in tasks such as reasoning over user given data, reading comprehension, math problem solving and text summarization. ',\n",
       " 'task': {'id': 'c329a1f9-323d-4e91-b2aa-582dd4188d34',\n",
       "  'name': 'Text Generation',\n",
       "  'description': 'Family of generative text models, such as large language models (LLM), that can be adapted for a variety of natural language tasks.'},\n",
       " 'tags': ['experimental', 'text-generation'],\n",
       " 'properties': [{'property_id': 'beta', 'value': 'true'},\n",
       "  {'property_id': 'constellation_config',\n",
       "   'value': 'max_concurrent_requests: 100'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_models():\n",
    "  api_url = f\"https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/models/search\"\n",
    "  response = requests.get(api_url, headers={\"Authorization\": f\"Bearer {API_TOKEN}\"})\n",
    "  all = response.json()\n",
    "  print(f\"There are {len(all['result'])} models available\")\n",
    "  hf_models = [m for m in all[\"result\"] if m[\"name\"].startswith(\"@hf\")]\n",
    "  print(f\"There are {len(hf_models)} Hugging Face models available\")\n",
    "  return hf_models\n",
    "\n",
    "hf = get_models()\n",
    "hf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thebloke/llamaguard-7b-awq',\n",
       " 'thebloke/neural-chat-7b-v3-1-awq',\n",
       " 'thebloke/orca-2-13b-awq',\n",
       " 'thebloke/codellama-7b-instruct-awq',\n",
       " 'thebloke/mistral-7b-instruct-v0.1-awq',\n",
       " 'thebloke/openchat_3.5-awq',\n",
       " 'thebloke/llama-2-13b-chat-awq',\n",
       " 'thebloke/deepseek-coder-6.7b-base-awq',\n",
       " 'thebloke/openhermes-2.5-mistral-7b-awq',\n",
       " 'thebloke/deepseek-coder-6.7b-instruct-awq',\n",
       " 'baai/bge-base-en-v1.5',\n",
       " 'thebloke/zephyr-7b-beta-awq']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = [m[\"name\"].replace(\"@hf/\",\"\") for m in hf]\n",
    "id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudflare Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def query(payload):\n",
    "  api_url = f\"https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@hf/thebloke/mistral-7b-instruct-v0.1-awq\"\n",
    "  response = requests.post(api_url, headers={\"Authorization\": f\"Bearer {API_TOKEN}\"}, json=payload)\n",
    "  return response.json()\n",
    "\n",
    "# Make sure to use the correct prompt & template, might vary based on model \n",
    "output = query({\n",
    "\t\"prompt\": \"Can you please let us know more details about your \",\n",
    "})\n",
    "print(output['result'][\"response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
