{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top 100 models for AWS partnership\n",
    "\n",
    "This create a CSV of the top 100 models from Hugging Face based on \"trending\" / \"30dlikes\" which we need to support in optimum and the cache. This mostly excludes any encoder models Additionally there are some not supported models. The CSV includes a column if supported in optimum and the cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering models:  31%|███       | 31/100 [01:30<02:33,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tags'\n",
      "Error parsing model stabilityai/sv3d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering models:  42%|████▏     | 42/100 [01:55<01:47,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tags'\n",
      "Error parsing model stabilityai/stable-video-diffusion-img2vid-xt-1-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering models: 100%|██████████| 100/100 [04:57<00:00,  2.98s/it]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 204.75ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11205"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as r\n",
    "from huggingface_hub import HfFolder\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset \n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {HfFolder.get_token()}\"}\n",
    "\n",
    "sess = r.Session()\n",
    "sess.headers.update(headers)\n",
    "\n",
    "SUPPORTED_ARCHITECTURES = [\n",
    "    \"llama\",\n",
    "    \"mistral\",\n",
    "    \"gpt2\",\n",
    "    \"clip\",\n",
    "    \"bloom\",\n",
    "    \"opt\" \"albert\",\n",
    "    \"bert\",\n",
    "    \"camembert\",\n",
    "    \"convbert\",\n",
    "    \"deberta\",\n",
    "    \"deberta-v2\",\n",
    "    \"distilbert\",\n",
    "    \"electra\",\n",
    "    \"esm\",\n",
    "    \"flaubert\",\n",
    "    \"mobilebert\",\n",
    "    \"mpnet\",\n",
    "    \"phi\",\n",
    "    \"roberta\",\n",
    "    \"roformer\",\n",
    "    \"xlm\",\n",
    "    \"xlm-roberta\",\n",
    "    \"t5\",\n",
    "    \"stable-diffusion\",\n",
    "    \"stable-diffusion-xl\",\n",
    "    \"latent-consistency\",\n",
    "]\n",
    "\n",
    "def get_architecture(model_id):\n",
    "  url = f\"https://huggingface.co/api/models/{model_id}\"\n",
    "  response = sess.get(url).json()\n",
    "  try: \n",
    "    return response[\"config\"][\"model_type\"]\n",
    "  except:\n",
    "    if \"stable-diffusion\" in response[\"tags\"]:\n",
    "      return \"stable-diffusion\"\n",
    "    elif \"stable-diffusion-xl\" in response[\"tags\"]:\n",
    "      return \"stable-diffusion-xl\"\n",
    "    else: \n",
    "        return \"N/A\"\n",
    "\n",
    "def is_model_cached(model_id):\n",
    "    # url= f\"https://optimum-neuron.huggingface.tech/lookup/{model_id}\"\n",
    "    url= f\"http://localhost:8000/lookup/{model_id}\"\n",
    "    try:\n",
    "        response = sess.get(url).json()\n",
    "        return True if len(response[\"cached_configs\"]) > 0 else False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_top_100_models(limit=100, type=\"likes30d\", filter=\"text-generation-inference\"):\n",
    "    url = f\"https://huggingface.co/api/models?sort={type}&direction=-1&limit={limit}\"\n",
    "    response = sess.get(url).json()\n",
    "    # map, filter list to remove gguf\n",
    "    filtered_models = []\n",
    "    for model in tqdm(response, desc=\"Filtering models\", total=len(response)):\n",
    "        try:\n",
    "            # get model architecture\n",
    "            arch  = get_architecture(model[\"id\"])\n",
    "            # filter supported architectures\n",
    "            supported = False\n",
    "            if arch in SUPPORTED_ARCHITECTURES:\n",
    "                supported = True\n",
    "            \n",
    "            # remove gguf models\n",
    "            if \"gguf\" in model[\"tags\"] and not \"text-generation-inference\" in model[\"tags\"]:\n",
    "                continue\n",
    "\n",
    "            # get license\n",
    "            license_value = next(\n",
    "                (\n",
    "                    tag.split(\":\", 1)[1]\n",
    "                    for tag in model[\"tags\"]\n",
    "                    if tag.startswith(\"license:\")\n",
    "                ),\n",
    "                \"N/A\",\n",
    "            )\n",
    "            _cached = is_model_cached(model[\"id\"])\n",
    "\n",
    "\n",
    "            # model size\n",
    "            filtered_models.append(\n",
    "                {\n",
    "                    \"model_id\": model[\"id\"],\n",
    "                    \"url\": f\"https://huggingface.co/{model['id']}\",\n",
    "                    \"architecture\": arch,\n",
    "                    \"supported\": supported,\n",
    "                    \"cached\": _cached,\n",
    "                    \"license\": license_value,\n",
    "                    \"likes30d\": model[\"likes30d\"],\n",
    "                    \"likes\": model[\"likes\"],\n",
    "                    \"downloads\": model[\"downloads\"],\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Error parsing model {model['id']}\")\n",
    "            continue\n",
    "    return filtered_models\n",
    "\n",
    "\n",
    "\n",
    "response = get_top_100_models()\n",
    "\n",
    "\n",
    "ds = Dataset.from_list(response)\n",
    "\n",
    "ds.to_csv(\"inf2_top_100.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 507.91ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11207"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
