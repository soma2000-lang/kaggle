{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud \n",
    "\n",
    "This notebook retrieves the top 500 models for Google Cloud and Vertex AI Garden including TGI configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dataclasses import asdict\n",
    "import requests as r \n",
    "from recommender.main import get_memory_per_model_and_tgi, get_tgi_config, get_quantization_type\n",
    "from recommender.utils.const import GOOGLE_CLOUD_INFERENCE_INSTANCE_TYPES\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {HfFolder.get_token()}\"}\n",
    "\n",
    "sess = r.Session()\n",
    "sess.headers.update(headers)\n",
    "\n",
    "\n",
    "def check_for_gate(model_id):\n",
    "  url = f\"https://huggingface.co/api/models/{model_id}\"\n",
    "  response = sess.get(url).json()\n",
    "  error = response.get(\"error\",None)\n",
    "  if error and \"gate\" in error:\n",
    "    return True\n",
    "  \n",
    "  return False\n",
    "\n",
    "\n",
    "def get_tgi_models_and_parse(limit=250,type=\"likes30d\",filter=\"text-generation-inference\"):\n",
    "  url=f\"https://huggingface.co/api/models?sort={type}&direction=-1&filter={filter}&limit={limit}\"\n",
    "  # url = \"https://huggingface.co/api/models/meta-llama/Llama-2-7b-chat-hf\"\n",
    "  # response = [r.get(url, headers=headers).json()]\n",
    "  response = sess.get(url).json()\n",
    "  # map, filter list to remove gguf \n",
    "  filtered_models=[]\n",
    "  for model in tqdm(response,total=len(response)):\n",
    "    try:\n",
    "      # backlist models which leads to crashes on my mac\n",
    "      if model[\"id\"] in [\"LargeWorldModel/LWM-Text-Chat-1M\",\"LargeWorldModel/LWM-Text-1M\",\"LargeWorldModel/LWM-Text-512K\", \"LargeWorldModel/LWM-Chat-512K\"]:\n",
    "        continue\n",
    "      \n",
    "      # remove gguf models\n",
    "      if \"gguf\" in model[\"tags\"]:\n",
    "        continue\n",
    "      \n",
    "      # check for gate\n",
    "      gated = check_for_gate(model[\"id\"])    \n",
    "      \n",
    "      # get license \n",
    "      license_value = next((tag.split(':', 1)[1] for tag in model[\"tags\"] if tag.startswith('license:')), \"N/A\")\n",
    "                  \n",
    "      # create TGI recommendation for Google Cloud \n",
    "      # potential_configs = []\n",
    "      for config in GOOGLE_CLOUD_INFERENCE_INSTANCE_TYPES[\"gpu\"]:\n",
    "        tgi_config = get_tgi_config(\n",
    "          model_id=model[\"id\"],\n",
    "          gpu_memory=config[\"memoryInGB\"],\n",
    "          num_gpus=config[\"numGpus\"],\n",
    "        )\n",
    "        if tgi_config:\n",
    "          config_dict = asdict(tgi_config)\n",
    "          config_dict[\"gcp_instance\"] = config[\"name\"]\n",
    "          break # break after first valid config to get smallest instance\n",
    "          # potential_configs.append(config_dict)\n",
    "\n",
    "            \n",
    "      # model size   \n",
    "      filtered_models.append({\n",
    "        \"model_id\": model[\"id\"],\n",
    "        \"url\": f\"https://huggingface.co/{model['id']}\",\n",
    "        \"cotaniner\": \"PyTorch TGI GPU\",\n",
    "        \"license\": license_value,\n",
    "        \"gated\": gated,\n",
    "        \"private\": model[\"private\"],\n",
    "        \"configuration\": config_dict,\n",
    "        \"likes\": model[\"likes\"],\n",
    "        \"likes30d\": model[\"likes30d\"],\n",
    "        \"downloads\": model[\"downloads\"],\n",
    "        \n",
    "      })\n",
    "    except Exception as e:\n",
    "      print(f\"Error parsing model {model['id']}\")\n",
    "      continue\n",
    "  return filtered_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mistralai/Mixtral-8x7B-Instruct-v0.1` from `transformers`...\n",
      "Loading pretrained config for `mistralai/Mixtral-8x7B-Instruct-v0.1` from `transformers`...\n",
      "Loading pretrained config for `mistralai/Mixtral-8x7B-Instruct-v0.1` from `transformers`...\n",
      "Loading pretrained config for `mistralai/Mixtral-8x7B-Instruct-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/500 [00:15<31:48,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `bigcode/starcoder2-15b` from `transformers`...\n",
      "Loading pretrained config for `bigcode/starcoder2-15b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [00:23<40:20,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meta-llama/Llama-2-7b-chat-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/500 [00:25<27:06,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mistralai/Mistral-7B-Instruct-v0.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/500 [00:27<25:08,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Genstruct-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/500 [00:29<22:11,  2.71s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Hermes-2-Pro-Mistral-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.42k/1.42k [00:00<00:00, 2.07MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 5.65MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 4.32MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 43.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `BioMistral/BioMistral-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 7.34k/7.34k [00:00<00:00, 3.73MB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.04MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.56MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 9.09MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.08k/1.08k [00:00<00:00, 2.19MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 137kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `microsoft/phi-2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 1.07MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 5.28MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.66MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 141kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mistralai/Mistral-7B-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 320/320 [00:00<00:00, 127kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 12.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 6.16MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `01-ai/Yi-9B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 516kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.9MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 112kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 443/443 [00:00<00:00, 1.16MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-Mistral-7B-DPO` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 833/833 [00:00<00:00, 842kB/s]\n",
      "tokenizer.json: 100%|██████████| 16.3M/16.3M [00:01<00:00, 12.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `CohereForAI/aya-101` from `transformers`...\n",
      "Loading pretrained config for `CohereForAI/aya-101` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 7.88k/7.88k [00:00<00:00, 12.6MB/s]\n",
      "vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 14.2MB/s]\n",
      "merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 14.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 8.92MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 958/958 [00:00<00:00, 1.83MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `bigcode/starcoder2-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 1.17MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 13.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.92MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meta-llama/Llama-2-7b-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 1.68MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 14.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 142kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mistralai/Mixtral-8x7B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `mistralai/Mixtral-8x7B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `mistralai/Mixtral-8x7B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `mistralai/Mixtral-8x7B-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 37.6k/37.6k [00:00<00:00, 404kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 4.40MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 3.16MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.07M/7.07M [00:00<00:00, 7.52MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 441/441 [00:00<00:00, 1.09MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Smaug-72B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-72B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-72B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-72B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-72B-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 20/500 [01:47<1:36:49, 12.10s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-DS-6.7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 21/500 [01:50<1:14:10,  9.29s/it]Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `HuggingFaceH4/zephyr-7b-gemma-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 3.18k/3.18k [00:00<00:00, 659kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 13.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 50.0/50.0 [00:00<00:00, 31.9kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 167kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `fireworks-ai/firefunction-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `fireworks-ai/firefunction-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `fireworks-ai/firefunction-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `fireworks-ai/firefunction-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 858kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 9.35MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 13.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 134kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `HuggingFaceTB/cosmo-1b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 1.17MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 9.60MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 60.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 709kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 7.88k/7.88k [00:00<00:00, 3.47MB/s]\n",
      "vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 10.8MB/s]\n",
      "merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 11.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 958/958 [00:00<00:00, 1.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `bigcode/starcoder2-3b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.14k/1.14k [00:00<00:00, 1.43MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 10.8MB/s]\n",
      "added_tokens.json: 100%|██████████| 60.0/60.0 [00:00<00:00, 118kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 233kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/ChatMusician` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.96k/1.96k [00:00<00:00, 7.81MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.38MB/s]\n",
      "added_tokens.json: 100%|██████████| 119/119 [00:00<00:00, 75.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 670/670 [00:00<00:00, 1.08MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ibm/merlinite-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 222/222 [00:00<00:00, 288kB/s]\n",
      "tokenizer.json: 100%|██████████| 14.5M/14.5M [00:01<00:00, 13.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 57.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `bigscience/bloom` from `transformers`...\n",
      "Loading pretrained config for `bigscience/bloom` from `transformers`...\n",
      "Loading pretrained config for `bigscience/bloom` from `transformers`...\n",
      "Loading pretrained config for `bigscience/bloom` from `transformers`...\n",
      "Loading pretrained config for `bigscience/bloom` from `transformers`...\n",
      "Loading pretrained config for `bigscience/bloom` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 1.12MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.9MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 15.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 63.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `HuggingFaceH4/zephyr-7b-beta` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 4.24k/4.24k [00:00<00:00, 4.10MB/s]\n",
      "tokenizer.json: 100%|██████████| 4.61M/4.61M [00:00<00:00, 11.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 462/462 [00:00<00:00, 523kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `gorilla-llm/gorilla-openfunctions-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 7.87k/7.87k [00:00<00:00, 3.63MB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 13.5MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.15MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 8.05MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.08k/1.08k [00:00<00:00, 4.45MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 473/473 [00:00<00:00, 667kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacaj/phi-2-super` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 2.61kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 15.0MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 14.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 12.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `openai-community/gpt2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 2.21MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 13.4MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 14.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 12.6MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 35/500 [03:42<1:20:04, 10.33s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `teknium/OpenHermes-2.5-Mistral-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 36/500 [03:45<1:04:26,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meta-llama/Llama-2-70b-chat-hf` from `transformers`...\n",
      "Loading pretrained config for `meta-llama/Llama-2-70b-chat-hf` from `transformers`...\n",
      "Loading pretrained config for `meta-llama/Llama-2-70b-chat-hf` from `transformers`...\n",
      "Loading pretrained config for `meta-llama/Llama-2-70b-chat-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.25k/1.25k [00:00<00:00, 3.73MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 6.13MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 6.83MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 624/624 [00:00<00:00, 1.58MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mlabonne/AlphaMonarch-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 7.67k/7.67k [00:00<00:00, 10.3MB/s]\n",
      "vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 4.57MB/s]\n",
      "merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 10.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 3.52MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 214kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 655/655 [00:00<00:00, 1.54MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `HuggingFaceH4/starchat2-15b-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `HuggingFaceH4/starchat2-15b-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.87k/1.87k [00:00<00:00, 3.10MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 3.23MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `deepseek-ai/deepseek-coder-33b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `deepseek-ai/deepseek-coder-33b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `deepseek-ai/deepseek-coder-33b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.99k/1.99k [00:00<00:00, 799kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 7.56MB/s]\n",
      "added_tokens.json: 100%|██████████| 119/119 [00:00<00:00, 163kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 556/556 [00:00<00:00, 614kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ibm/labradorite-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ibm/labradorite-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 677/677 [00:00<00:00, 1.37MB/s]\n",
      "vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 1.91MB/s]\n",
      "merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 12.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 3.56MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 532/532 [00:00<00:00, 326kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `bigcode/starcoder` from `transformers`...\n",
      "Loading pretrained config for `bigcode/starcoder` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 322/322 [00:00<00:00, 12.5kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 17.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 5.87MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `01-ai/Yi-34B-200K` from `transformers`...\n",
      "Loading pretrained config for `01-ai/Yi-34B-200K` from `transformers`...\n",
      "Loading pretrained config for `01-ai/Yi-34B-200K` from `transformers`...\n",
      "Loading pretrained config for `01-ai/Yi-34B-200K` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 635kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 21.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.26MB/s]\n",
      "added_tokens.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 84.1kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 491/491 [00:00<00:00, 1.04MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `openchat/openchat-3.5-0106` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 1.03MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 5.50MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 141kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TencentARC/Mistral_Pro_8B_v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 971/971 [00:00<00:00, 491kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 40.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 461kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `yam-peleg/Experiment26-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 614kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 13.1MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 13.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 13.4MB/s]\n",
      "added_tokens.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 227kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 367/367 [00:00<00:00, 1.03MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Liberated-Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Liberated-Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Liberated-Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Liberated-Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Liberated-Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 289kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.1MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 6.92kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 153kB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `georgesung/llama2_7b_chat_uncensored` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.70k/1.70k [00:00<00:00, 790kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.2MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 16.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 443/443 [00:00<00:00, 279kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.5-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.5-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.5-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.5-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 4.26k/4.26k [00:00<00:00, 2.26MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 2.72MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 482/482 [00:00<00:00, 760kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-DS-33B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-DS-33B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-DS-33B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 953/953 [00:00<00:00, 938kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.37MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 184kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 145/145 [00:00<00:00, 294kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `BatsResearch/bonito-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 824/824 [00:00<00:00, 801kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 434/434 [00:00<00:00, 875kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Phind/Phind-CodeLlama-34B-v2` from `transformers`...\n",
      "Loading pretrained config for `Phind/Phind-CodeLlama-34B-v2` from `transformers`...\n",
      "Loading pretrained config for `Phind/Phind-CodeLlama-34B-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.29k/1.29k [00:00<00:00, 4.88MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 12.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 1.87MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TinyLlama/TinyLlama-1.1B-Chat-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.47k/1.47k [00:00<00:00, 2.55MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 9.26MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 138kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mistralai/Mistral-7B-Instruct-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 663/663 [00:00<00:00, 405kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 320/320 [00:00<00:00, 784kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 12.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 5.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `01-ai/Yi-9B-200K` from `transformers`...\n",
      "Loading pretrained config for `01-ai/Yi-9B-200K` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 336kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 9.99MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 96.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meta-llama/Llama-2-13b-chat-hf` from `transformers`...\n",
      "Loading pretrained config for `meta-llama/Llama-2-13b-chat-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 3.20MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 11.4MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 11.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 12.0MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-7B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 1.33MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 17.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.9MB/s]\n",
      "added_tokens.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 48.0kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 502/502 [00:00<00:00, 322kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `FuseAI/FuseChat-7B-VaRM` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 8.92k/8.92k [00:00<00:00, 3.61MB/s]\n",
      "vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 15.2MB/s]\n",
      "merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 1.46MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 11.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 13.4kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.44k/1.44k [00:00<00:00, 598kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphincoder-starcoder2-15b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphincoder-starcoder2-15b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 1.24MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.00M/1.00M [00:00<00:00, 11.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 465kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Russian-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.96k/1.96k [00:00<00:00, 633kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 538/538 [00:00<00:00, 289kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WhiteRabbitNeo/WhiteRabbitNeo-13B-v1` from `transformers`...\n",
      "Loading pretrained config for `WhiteRabbitNeo/WhiteRabbitNeo-13B-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 66/500 [07:48<32:18,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model Yellow-AI-NLP/komodo-7b-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.10k/2.10k [00:00<00:00, 734kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 169kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `AetherResearch/Cerebrum-1.0-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.87k/1.87k [00:00<00:00, 923kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 12.1MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `deepseek-ai/deepseek-coder-6.7b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 2.65MB/s]\n",
      "tokenizer.model: 100%|██████████| 986k/986k [00:00<00:00, 14.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 823kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Arabic-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 2.80MB/s]\n",
      "tokenizer.model: 100%|██████████| 887k/887k [00:00<00:00, 6.34MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 585kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Turkish-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 956/956 [00:00<00:00, 1.72MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.34MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 73.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 216kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `intfloat/e5-mistral-7b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 966/966 [00:00<00:00, 1.12MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 386kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Sao10K/Fimbulvetr-11B-v2` from `transformers`...\n",
      "Loading pretrained config for `Sao10K/Fimbulvetr-11B-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.04k/2.04k [00:00<00:00, 1.41MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 1.67MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 15.8kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 443/443 [00:00<00:00, 828kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Weyaxi/Einstein-v4-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.90k/1.90k [00:00<00:00, 3.94MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 3.35MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 662/662 [00:00<00:00, 2.59MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `01-ai/Yi-34B-Chat` from `transformers`...\n",
      "Loading pretrained config for `01-ai/Yi-34B-Chat` from `transformers`...\n",
      "Loading pretrained config for `01-ai/Yi-34B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 1.72MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 3.40MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 3.18MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 12.9MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-0.5B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 1.15MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 20.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 12.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 548/548 [00:00<00:00, 458kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `wolfram/miquliz-120b-v2.0` from `transformers`...\n",
      "Loading pretrained config for `wolfram/miquliz-120b-v2.0` from `transformers`...\n",
      "Loading pretrained config for `wolfram/miquliz-120b-v2.0` from `transformers`...\n",
      "Loading pretrained config for `wolfram/miquliz-120b-v2.0` from `transformers`...\n",
      "Loading pretrained config for `wolfram/miquliz-120b-v2.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 963/963 [00:00<00:00, 217kB/s]\n",
      "tokenizer.model: 100%|██████████| 986k/986k [00:00<00:00, 15.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.61M/3.61M [00:00<00:00, 13.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 21.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Arabic-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 963/963 [00:00<00:00, 312kB/s]\n",
      "tokenizer.model: 100%|██████████| 887k/887k [00:00<00:00, 15.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.34M/3.34M [00:00<00:00, 6.45MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 19.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Turkish-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 774/774 [00:00<00:00, 249kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.10k/2.10k [00:00<00:00, 747kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 4.10MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 127kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `AetherResearch/Cerebrum-1.0-8x7b` from `transformers`...\n",
      "Loading pretrained config for `AetherResearch/Cerebrum-1.0-8x7b` from `transformers`...\n",
      "Loading pretrained config for `AetherResearch/Cerebrum-1.0-8x7b` from `transformers`...\n",
      "Loading pretrained config for `AetherResearch/Cerebrum-1.0-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 671/671 [00:00<00:00, 299kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.63k/1.63k [00:00<00:00, 3.64MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.2MB/s]\n",
      "added_tokens.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 179kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 536/536 [00:00<00:00, 955kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Nexusflow/Starling-LM-7B-beta` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.61k/1.61k [00:00<00:00, 4.96MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.34MB/s]\n",
      "added_tokens.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 94.3kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 560/560 [00:00<00:00, 575kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `berkeley-nest/Starling-LM-7B-alpha` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 5.02MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 13.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 12.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n",
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 82/500 [09:54<1:09:24,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `roborovski/superprompt-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 2.79MB/s]\n",
      "tokenizer.model: 100%|██████████| 874k/874k [00:00<00:00, 8.37MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 2.84MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Hungarian-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 4.12MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 13.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 4.11MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 3.88MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-large` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.17k/2.17k [00:00<00:00, 4.10MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 7.35MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 1.30MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `openchat/openchat-3.5-0106-gemma` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 331kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.7MB/s]\n",
      "added_tokens.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 16.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 491/491 [00:00<00:00, 151kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `openchat/openchat_3.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 320/320 [00:00<00:00, 252kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 12.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 5.25MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `01-ai/Yi-34B` from `transformers`...\n",
      "Loading pretrained config for `01-ai/Yi-34B` from `transformers`...\n",
      "Loading pretrained config for `01-ai/Yi-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.41k/1.41k [00:00<00:00, 475kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 16.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `upstage/SOLAR-10.7B-Instruct-v1.0` from `transformers`...\n",
      "Loading pretrained config for `upstage/SOLAR-10.7B-Instruct-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 4.72k/4.72k [00:00<00:00, 1.63MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 2.73MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 573/573 [00:00<00:00, 357kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WhiteRabbitNeo/WhiteRabbitNeo-7B-v1.5a` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.02k/1.02k [00:00<00:00, 327kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 16.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 147kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `l3utterfly/mistral-7b-v0.1-layla-v4` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 613kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 2.86MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 768kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/bigstral-12b-32k` from `transformers`...\n",
      "Loading pretrained config for `abacusai/bigstral-12b-32k` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.15k/2.15k [00:00<00:00, 680kB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 13.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 11.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 180kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `yam-peleg/Hebrew-Gemma-11B` from `transformers`...\n",
      "Loading pretrained config for `yam-peleg/Hebrew-Gemma-11B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 675kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.36MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 420kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Equall/Saul-Instruct-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 803kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.01M/1.01M [00:00<00:00, 13.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 981kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Bulgarian-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 1.05MB/s]\n",
      "tokenizer.model: 100%|██████████| 897k/897k [00:00<00:00, 14.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 2.10MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Japanese-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 3.14MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.10M/1.10M [00:00<00:00, 11.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 2.66MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Thai-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 963/963 [00:00<00:00, 4.17MB/s]\n",
      "tokenizer.model: 100%|██████████| 874k/874k [00:00<00:00, 12.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.20M/3.20M [00:00<00:00, 11.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 255kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Hungarian-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 963/963 [00:00<00:00, 745kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.00M/1.00M [00:00<00:00, 15.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 5.29MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 67.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Russian-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 8.78MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 13.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 13.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 3.13MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 642/642 [00:00<00:00, 968kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 7.02MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 823kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `HuggingFaceH4/mistral-7b-grok` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 963/963 [00:00<00:00, 131kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.10M/1.10M [00:00<00:00, 8.39MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.64M/3.64M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 251kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Thai-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 2.06MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 12.2MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 13.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 7.21MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-14B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-14B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 918/918 [00:00<00:00, 214kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 17.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 5.29MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 53.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `MBZUAI/MobiLlama-05B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 495kB/s]\n",
      "tokenizer.model: 100%|██████████| 893k/893k [00:00<00:00, 13.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 200kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Slovenian-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.39k/1.39k [00:00<00:00, 156kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.01M/1.01M [00:00<00:00, 11.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 135kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Serbian-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 789kB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 13.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 3.95MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 687kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xxl` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 208kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 11.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 39.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meta-llama/Llama-2-70b-hf` from `transformers`...\n",
      "Loading pretrained config for `meta-llama/Llama-2-70b-hf` from `transformers`...\n",
      "Loading pretrained config for `meta-llama/Llama-2-70b-hf` from `transformers`...\n",
      "Loading pretrained config for `meta-llama/Llama-2-70b-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 4.24k/4.24k [00:00<00:00, 1.63MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 3.11MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 442/442 [00:00<00:00, 33.4kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `PipableAI/pip-sql-1.3b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 963/963 [00:00<00:00, 215kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.01M/1.01M [00:00<00:00, 15.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.57M/3.57M [00:00<00:00, 5.14MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 12.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Serbian-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.57k/1.57k [00:00<00:00, 474kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 14.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 570/570 [00:00<00:00, 162kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-Yi-34B` from `transformers`...\n",
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-Yi-34B` from `transformers`...\n",
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-Yi-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 963/963 [00:00<00:00, 302kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.01M/1.01M [00:00<00:00, 10.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.61M/3.61M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 47.8kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Bulgarian-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 362kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 2.60MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 12.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 548/548 [00:00<00:00, 349kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sophosympatheia/Midnight-Miqu-70B-v1.0` from `transformers`...\n",
      "Loading pretrained config for `sophosympatheia/Midnight-Miqu-70B-v1.0` from `transformers`...\n",
      "Loading pretrained config for `sophosympatheia/Midnight-Miqu-70B-v1.0` from `transformers`...\n",
      "Loading pretrained config for `sophosympatheia/Midnight-Miqu-70B-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 406kB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 11.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 12.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 555/555 [00:00<00:00, 82.8kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `beomi/gemma-ko-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 237/237 [00:00<00:00, 73.0kB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 14.5MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 15.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 12.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.08k/1.08k [00:00<00:00, 308kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 36.3kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `microsoft/phi-1_5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 932/932 [00:00<00:00, 273kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.41MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 91.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codellama/CodeLlama-70b-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-70b-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-70b-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-70b-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 963/963 [00:00<00:00, 322kB/s]\n",
      "tokenizer.model: 100%|██████████| 897k/897k [00:00<00:00, 3.06MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.03M/3.03M [00:00<00:00, 4.73MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 15.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Japanese-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 963/963 [00:00<00:00, 179kB/s]\n",
      "tokenizer.model: 100%|██████████| 893k/893k [00:00<00:00, 16.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.44M/3.44M [00:00<00:00, 12.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 29.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/SambaLingo-Slovenian-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 935/935 [00:00<00:00, 533kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 2.33MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 13.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 147kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `OPI-PG/Qra-13b` from `transformers`...\n",
      "Loading pretrained config for `OPI-PG/Qra-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 409kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 10.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 140kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `lmsys/vicuna-7b-v1.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.69k/1.69k [00:00<00:00, 898kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 1.34MB/s]\n",
      "added_tokens.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 10.4kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 101/101 [00:00<00:00, 12.9kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Open-Orca/Mistral-7B-OpenOrca` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 947/947 [00:00<00:00, 1.59MB/s]\n",
      " 25%|██▍       | 123/500 [20:36<10:15:32, 97.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model NousResearch/Nous-Capybara-34B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.42k/1.42k [00:00<00:00, 2.09MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 5.78MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.38MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 84.5kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 242/242 [00:00<00:00, 1.08MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `snorkelai/Snorkel-Mistral-PairRM-DPO` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 287/287 [00:00<00:00, 590kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 4.49MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 375kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `tiiuae/falcon-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 966/966 [00:00<00:00, 1.30MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.23MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 284kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `upstage/SOLAR-10.7B-v1.0` from `transformers`...\n",
      "Loading pretrained config for `upstage/SOLAR-10.7B-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.01k/1.01k [00:00<00:00, 3.68MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 10.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 511/511 [00:00<00:00, 1.57MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n",
      "Loading pretrained config for `senseable/WestLake-7B-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 37.6kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 548/548 [00:00<00:00, 924kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `152334H/miqu-1-70b-sf` from `transformers`...\n",
      "Loading pretrained config for `152334H/miqu-1-70b-sf` from `transformers`...\n",
      "Loading pretrained config for `152334H/miqu-1-70b-sf` from `transformers`...\n",
      "Loading pretrained config for `152334H/miqu-1-70b-sf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.77k/1.77k [00:00<00:00, 556kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 6.00MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 5.11MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 640/640 [00:00<00:00, 200kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `CausalLM/34b-beta` from `transformers`...\n",
      "Loading pretrained config for `CausalLM/34b-beta` from `transformers`...\n",
      "Loading pretrained config for `CausalLM/34b-beta` from `transformers`...\n",
      "Loading pretrained config for `CausalLM/34b-beta` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.86k/1.86k [00:00<00:00, 1.03MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.18M/2.18M [00:00<00:00, 13.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 781kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `yanolja/EEVE-Korean-10.8B-v1.0` from `transformers`...\n",
      "Loading pretrained config for `yanolja/EEVE-Korean-10.8B-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 133/500 [22:10<1:35:13, 15.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model amazon/chronos-t5-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 994/994 [00:00<00:00, 1.15MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 11.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 1.92MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-CL-70B` from `transformers`...\n",
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-CL-70B` from `transformers`...\n",
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-CL-70B` from `transformers`...\n",
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-CL-70B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.02k/1.02k [00:00<00:00, 3.76MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 17.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 532kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `gordicaleksa/YugoGPT` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.70k/1.70k [00:00<00:00, 1.35MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.4MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 72.2kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 443/443 [00:00<00:00, 529kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.6-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.6-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.6-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.6-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 851kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 443/443 [00:00<00:00, 770kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-SOLAR-10.7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Nous-Hermes-2-SOLAR-10.7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 138/500 [22:59<54:12,  8.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model MediaTek-Research/Breexe-8x7B-Instruct-v0_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.64k/1.64k [00:00<00:00, 2.66MB/s]\n",
      "vocab.json: 100%|██████████| 3.38M/3.38M [00:00<00:00, 9.73MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 3.14MB/s]\n",
      "added_tokens.json: 100%|██████████| 103/103 [00:00<00:00, 214kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 363/363 [00:00<00:00, 1.32MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Crystalcareai/Qwen1.5-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Crystalcareai/Qwen1.5-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Crystalcareai/Qwen1.5-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Crystalcareai/Qwen1.5-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 668/668 [00:00<00:00, 555kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.41k/1.41k [00:00<00:00, 2.57MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 10.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.34MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.74MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `kaist-ai/mistral-orpo-beta` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 645/645 [00:00<00:00, 494kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.38k/1.38k [00:00<00:00, 2.30MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.32MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 623kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Replete-AI/Mistral-Evolved-11b-v0.1` from `transformers`...\n",
      "Loading pretrained config for `Replete-AI/Mistral-Evolved-11b-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 144/500 [23:30<27:01,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model CausalLM/14B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 431kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 4.36MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 11.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 13.3MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-0.5B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.17k/1.17k [00:00<00:00, 120kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 10.7MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 3.20MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 13.2MB/s]\n",
      "added_tokens.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 33.3kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 370/370 [00:00<00:00, 275kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sail/Sailor-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 255/255 [00:00<00:00, 130kB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 13.4MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 17.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 9.79MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 130kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Gustavosta/MagicPrompt-Stable-Diffusion` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 148/500 [23:47<22:58,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model TheBloke/Llama-2-7B-Chat-GGML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 214/214 [00:00<00:00, 48.3kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 11.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 594kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `tiiuae/falcon-180B` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-180B` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-180B` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-180B` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-180B` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-180B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 150/500 [23:59<26:18,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model epfl-llm/meditron-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 610/610 [00:00<00:00, 929kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 5.33k/5.33k [00:00<00:00, 4.23MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 323kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meta-llama/LlamaGuard-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.29k/2.29k [00:00<00:00, 2.09MB/s]\n",
      "tokenizer.model: 100%|██████████| 911k/911k [00:00<00:00, 11.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.79M/2.79M [00:00<00:00, 10.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 39.0/39.0 [00:00<00:00, 191kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 1.83MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `MediaTek-Research/Breeze-7B-Instruct-v0_1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.74k/1.74k [00:00<00:00, 1.53MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.2MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 111kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 741/741 [00:00<00:00, 2.83MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `dreamgen/opus-v1.2-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 154/500 [24:12<17:58,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model marathi-llm/MahaMarathi-7B-v24.01-Base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 67.9kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 4.84MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 10.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 843kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meta-llama/Llama-2-13b-hf` from `transformers`...\n",
      "Loading pretrained config for `meta-llama/Llama-2-13b-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 828/828 [00:00<00:00, 1.66MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 16.8MB/s]\n",
      "added_tokens.json: 100%|██████████| 69.0/69.0 [00:00<00:00, 146kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 1.77MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `microsoft/Orca-2-13b` from `transformers`...\n",
      "Loading pretrained config for `microsoft/Orca-2-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 6.92MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 21.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 46.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ` from `transformers`...\n",
      "Loading pretrained config for `TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.70k/1.70k [00:00<00:00, 637kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.1MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 118kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 443/443 [00:00<00:00, 1.58MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.7-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.7-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.7-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.7-mixtral-8x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 106/106 [00:00<00:00, 60.5kB/s]\n",
      "tokenizer.json: 100%|██████████| 3.42M/3.42M [00:00<00:00, 5.04MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 124kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `riotu-lab/ArabianGPT-03B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 932/932 [00:00<00:00, 3.57MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 12.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 862kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `defog/sqlcoder-70b-alpha` from `transformers`...\n",
      "Loading pretrained config for `defog/sqlcoder-70b-alpha` from `transformers`...\n",
      "Loading pretrained config for `defog/sqlcoder-70b-alpha` from `transformers`...\n",
      "Loading pretrained config for `defog/sqlcoder-70b-alpha` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.50k/1.50k [00:00<00:00, 999kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 9.59MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 5.29MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 635kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `jondurbin/airoboros-34b-3.2` from `transformers`...\n",
      "Loading pretrained config for `jondurbin/airoboros-34b-3.2` from `transformers`...\n",
      "Loading pretrained config for `jondurbin/airoboros-34b-3.2` from `transformers`...\n",
      "Loading pretrained config for `jondurbin/airoboros-34b-3.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 124kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.22MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 46.5kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 482kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/WizardLM-7B-Uncensored` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 3.30MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 16.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.75MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 20.8k/20.8k [00:00<00:00, 20.5MB/s]\n",
      "spiece.model: 100%|██████████| 1.47M/1.47M [00:00<00:00, 12.3MB/s]\n",
      "added_tokens.json: 100%|██████████| 2.59k/2.59k [00:00<00:00, 679kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 5.32MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_translate_en_ru_zh_large_1024` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 2.91MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 22.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.52MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Equall/Saul-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 971/971 [00:00<00:00, 579kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.21MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 1.11MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `bardsai/jaskier-7b-dpo-v5.6` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.10k/1.10k [00:00<00:00, 477kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 10.9MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 44.0kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 169/169 [00:00<00:00, 156kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `MBZUAI/MobiLlama-1B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.04k/2.04k [00:00<00:00, 4.40MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.1MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 59.1kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 558/558 [00:00<00:00, 1.18MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.8-experiment26-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.15k/2.15k [00:00<00:00, 164kB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 6.77MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 11.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 226kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `yam-peleg/Hebrew-Gemma-11B-Instruct` from `transformers`...\n",
      "Loading pretrained config for `yam-peleg/Hebrew-Gemma-11B-Instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 55.5kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.48M/1.48M [00:00<00:00, 4.56MB/s]\n",
      "tokenizer.json: 100%|██████████| 5.75M/5.75M [00:00<00:00, 11.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 157kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `saltlux/luxia-21.4b-alignment-v1.0` from `transformers`...\n",
      "Loading pretrained config for `saltlux/luxia-21.4b-alignment-v1.0` from `transformers`...\n",
      "Loading pretrained config for `saltlux/luxia-21.4b-alignment-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 685/685 [00:00<00:00, 562kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.02k/1.02k [00:00<00:00, 343kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 1.72MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 121kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheDrummer/Moistral-11B-v1` from `transformers`...\n",
      "Loading pretrained config for `TheDrummer/Moistral-11B-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 6.80kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 11.5MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.49MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 9.16MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `distilbert/distilgpt2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 287/287 [00:00<00:00, 22.5kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 4.42MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 27.7kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `tiiuae/falcon-7b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 657/657 [00:00<00:00, 264kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 770/770 [00:00<00:00, 243kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.31MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 158kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `FlagAlpha/Llama2-Chinese-7b-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 1.98MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 12.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 143kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/text_summarization` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 953/953 [00:00<00:00, 302kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 4.61MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 8.57MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 17.2kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 145/145 [00:00<00:00, 69.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Yarn-Mistral-7b-128k` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 178/500 [27:43<27:34,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model epfl-llm/meditron-70b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.90k/1.90k [00:00<00:00, 631kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 16.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 662/662 [00:00<00:00, 310kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `01-ai/Yi-6B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.12k/2.12k [00:00<00:00, 881kB/s]\n",
      "tokenizer.model: 100%|██████████| 718k/718k [00:00<00:00, 20.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 547/547 [00:00<00:00, 2.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Trendyol/Trendyol-LLM-7b-chat-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 126kB/s]\n",
      "tokenizer.model: 100%|██████████| 903k/903k [00:00<00:00, 15.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 373kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `tokyotech-llm/Swallow-MS-7b-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.35k/1.35k [00:00<00:00, 2.38MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 17.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.27MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 436/436 [00:00<00:00, 2.28MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `GritLM/GritLM-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 183/500 [28:01<18:20,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model Nexusflow/Starling-RM-34B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 653/653 [00:00<00:00, 356kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.58k/1.58k [00:00<00:00, 1.33MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.21MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 4.16kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 550/550 [00:00<00:00, 471kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ContextualAI/Contextual_KTO_Mistral_PairRM` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.45k/1.45k [00:00<00:00, 2.13MB/s]\n",
      "tokenizer.model: 100%|██████████| 712k/712k [00:00<00:00, 17.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 944kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Trendyol/Trendyol-LLM-7b-chat-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 442kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 26.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codellama/CodeLlama-7b-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 482kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 16.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 74.5kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 402kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `HuggingFaceH4/zephyr-7b-alpha` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 631/631 [00:00<00:00, 443kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.87k/1.87k [00:00<00:00, 2.51MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 12.1MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `deepseek-ai/deepseek-coder-1.3b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.83k/1.83k [00:00<00:00, 3.84MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 11.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 482/482 [00:00<00:00, 47.5kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codefuse-ai/CodeFuse-DeepSeek-33B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codefuse-ai/CodeFuse-DeepSeek-33B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codefuse-ai/CodeFuse-DeepSeek-33B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 191/500 [28:36<21:36,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model Viet-Mistral/Vistral-7B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.41k/1.41k [00:00<00:00, 629kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 19.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 826/826 [00:00<00:00, 1.71MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `h2oai/h2o-danube-1.8b-chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.80k/1.80k [00:00<00:00, 8.81MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 18.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 5.55MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 255kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 1.34MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/TheProfessor-155b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/TheProfessor-155b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/TheProfessor-155b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/TheProfessor-155b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/TheProfessor-155b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/TheProfessor-155b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 5.81k/5.81k [00:00<00:00, 6.50MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 12.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 573/573 [00:00<00:00, 2.73MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WhiteRabbitNeo/WhiteRabbitNeo-33B-v1.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WhiteRabbitNeo/WhiteRabbitNeo-33B-v1.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WhiteRabbitNeo/WhiteRabbitNeo-33B-v1.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 4.52MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.46MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 548/548 [00:00<00:00, 1.35MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `AlexWortega/miqu-1-70b-AQLM-2Bit-1x16-hf` from `transformers`...\n",
      "Loading pretrained config for `AlexWortega/miqu-1-70b-AQLM-2Bit-1x16-hf` from `transformers`...\n",
      "Loading pretrained config for `AlexWortega/miqu-1-70b-AQLM-2Bit-1x16-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.34k/2.34k [00:00<00:00, 1.53MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 13.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 12.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 635/635 [00:00<00:00, 455kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `IlyaGusev/saiga_gemma_9b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 198kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 16.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.50MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 548/548 [00:00<00:00, 186kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `wolfram/miqu-1-103b` from `transformers`...\n",
      "Loading pretrained config for `wolfram/miqu-1-103b` from `transformers`...\n",
      "Loading pretrained config for `wolfram/miqu-1-103b` from `transformers`...\n",
      "Loading pretrained config for `wolfram/miqu-1-103b` from `transformers`...\n",
      "Loading pretrained config for `wolfram/miqu-1-103b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 1.97MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 11.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 2.75MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-small` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 242/242 [00:00<00:00, 76.2kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 12.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 88.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `tiiuae/falcon-40b` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-40b` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-40b` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-40b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 9.67kB/s]\n",
      " 40%|████      | 202/500 [30:26<39:39,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model TheBloke/Llama-2-13B-chat-GGML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 822kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 4.38MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 13.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 7.56MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.87k/1.87k [00:00<00:00, 656kB/s]\n",
      "tokenizer.json: 100%|██████████| 4.61M/4.61M [00:00<00:00, 12.5MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `deepseek-ai/deepseek-coder-7b-instruct-v1.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 5.39k/5.39k [00:00<00:00, 846kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 15.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 462/462 [00:00<00:00, 132kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-DS-1.3B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 207/500 [30:59<27:13,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model zhengr/MixTAO-7Bx2-MoE-v8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.44k/1.44k [00:00<00:00, 368kB/s]\n",
      "tokenizer.model: 100%|██████████| 624k/624k [00:00<00:00, 13.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.21M/2.21M [00:00<00:00, 3.95MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 199kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `INSAIT-Institute/BgGPT-7B-Instruct-v0.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 745/745 [00:00<00:00, 129kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 8.13k/8.13k [00:00<00:00, 6.60MB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 16.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.12M/2.12M [00:00<00:00, 13.3MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 1.11MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 586/586 [00:00<00:00, 89.1kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `rhysjones/phi-2-orange-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 696/696 [00:00<00:00, 890kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.33k/2.33k [00:00<00:00, 4.89MB/s]\n",
      "tokenizer.model: 100%|██████████| 911k/911k [00:00<00:00, 12.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.79M/2.79M [00:00<00:00, 4.40MB/s]\n",
      "added_tokens.json: 100%|██████████| 39.0/39.0 [00:00<00:00, 194kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 127kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `MediaTek-Research/Breeze-7B-Instruct-v1_0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 709/709 [00:00<00:00, 286kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 140kB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 13.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:02<00:00, 6.89MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 555/555 [00:00<00:00, 181kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `FreedomIntelligence/Apollo-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 976/976 [00:00<00:00, 346kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.38k/1.38k [00:00<00:00, 2.64MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.45MB/s]\n",
      "added_tokens.json: 100%|██████████| 59.0/59.0 [00:00<00:00, 129kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 772/772 [00:00<00:00, 1.35MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ezelikman/quietstar-8-ahead` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 11.9kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.32MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.45MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 9.44MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `openai-community/gpt2-xl` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 746/746 [00:00<00:00, 1.06MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 13.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 709kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Llama-2-7b-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 746/746 [00:00<00:00, 654kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 13.4MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 44.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 759kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Llama-2-7b-chat-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.57k/1.57k [00:00<00:00, 5.70MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.35MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 12.9MB/s]\n",
      "added_tokens.json: 100%|██████████| 92.0/92.0 [00:00<00:00, 188kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 207/207 [00:00<00:00, 796kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `HuggingFaceM4/idefics-9b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 953/953 [00:00<00:00, 4.65MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 8.21MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 76.3kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 145/145 [00:00<00:00, 718kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mychen76/mistral7b_ocr_to_json_v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 697/697 [00:00<00:00, 918kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 286/286 [00:00<00:00, 1.39MB/s]\n",
      "tokenizer.json: 100%|██████████| 5.64M/5.64M [00:00<00:00, 11.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 545/545 [00:00<00:00, 1.13MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `LumiOpen/Poro-34B` from `transformers`...\n",
      "Loading pretrained config for `LumiOpen/Poro-34B` from `transformers`...\n",
      "Loading pretrained config for `LumiOpen/Poro-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 916/916 [00:00<00:00, 582kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 22.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 13.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 201kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `SanjiWatsuki/Kunoichi-DPO-v2-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 568kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 12.1MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 3.15MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 13.1MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-4B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 561kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 13.3MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 12.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 13.8MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B-Chat-GPTQ-Int4` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B-Chat-GPTQ-Int4` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-72B-Chat-GPTQ-Int4` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.35k/1.35k [00:00<00:00, 1.01MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 15.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 13.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 573/573 [00:00<00:00, 184kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `brucethemoose/Yi-34B-200K-RPMerge` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `brucethemoose/Yi-34B-200K-RPMerge` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `brucethemoose/Yi-34B-200K-RPMerge` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `brucethemoose/Yi-34B-200K-RPMerge` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 671/671 [00:00<00:00, 173kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 675k/675k [00:00<00:00, 13.1MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 18.4MB/s]\n",
      "added_tokens.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 44.6kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 563/563 [00:00<00:00, 786kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/fc-dolphin-2.6-mistral-7b-dpo-laser` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 537/537 [00:00<00:00, 342kB/s]\n",
      "vocab.json: 100%|██████████| 927k/927k [00:00<00:00, 15.9MB/s]\n",
      "merges.txt: 100%|██████████| 585k/585k [00:00<00:00, 17.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.37M/2.37M [00:00<00:00, 14.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 645kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ytu-ce-cosmos/turkish-gpt2-large` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.44k/1.44k [00:00<00:00, 1.35MB/s]\n",
      "tokenizer.model: 100%|██████████| 624k/624k [00:00<00:00, 15.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.21M/2.21M [00:00<00:00, 12.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 1.14MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `INSAIT-Institute/BgGPT-7B-Instruct-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 775/775 [00:00<00:00, 159kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 566kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 40.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 194kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1` from `transformers`...\n",
      "Loading pretrained config for `tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1` from `transformers`...\n",
      "Loading pretrained config for `tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1` from `transformers`...\n",
      "Loading pretrained config for `tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 752/752 [00:00<00:00, 1.58MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 935/935 [00:00<00:00, 1.74MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.42MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 1.59MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `OPI-PG/Qra-1b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 714/714 [00:00<00:00, 139kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 3.44MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 13.8MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 18.1kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 1.09MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `migtissera/Tess-70B-v1.6` from `transformers`...\n",
      "Loading pretrained config for `migtissera/Tess-70B-v1.6` from `transformers`...\n",
      "Loading pretrained config for `migtissera/Tess-70B-v1.6` from `transformers`...\n",
      "Loading pretrained config for `migtissera/Tess-70B-v1.6` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.98k/1.98k [00:00<00:00, 214kB/s]\n",
      " 46%|████▌     | 229/500 [33:39<38:33,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model stanford-crfm/music-large-800k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 7.88kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 12.6MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.47MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 11.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `openai-community/gpt2-large` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.90MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 13.8MB/s]\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/hf/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n",
      "Loading pretrained config for `google-t5/t5-base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 752/752 [00:00<00:00, 549kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.86k/1.86k [00:00<00:00, 565kB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 14.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 549kB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n",
      "Loading pretrained config for `DeepFloyd/t5-v1_1-xxl` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 782/782 [00:00<00:00, 279kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 933/933 [00:00<00:00, 827kB/s]\n",
      "vocab.json: 100%|██████████| 1.77M/1.77M [00:00<00:00, 8.07MB/s]\n",
      "merges.txt: 100%|██████████| 1.23M/1.23M [00:00<00:00, 13.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 582/582 [00:00<00:00, 184kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ai-forever/ruGPT-3.5-13B` from `transformers`...\n",
      "Loading pretrained config for `ai-forever/ruGPT-3.5-13B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 185kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.49MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 4.39kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 33.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/WizardLM-13B-Uncensored` from `transformers`...\n",
      "Loading pretrained config for `cognitivecomputations/WizardLM-13B-Uncensored` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 601kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 10.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 118kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ` from `transformers`...\n",
      "Loading pretrained config for `TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 766/766 [00:00<00:00, 310kB/s]\n",
      "tokenizer.model: 100%|██████████| 844k/844k [00:00<00:00, 1.84MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 282kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `hfl/chinese-alpaca-2-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 487kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 1.38kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 215kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/MythoMax-L2-13B-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 660/660 [00:00<00:00, 272kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 721/721 [00:00<00:00, 208kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.7MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 6.27kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 32.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WizardLM/WizardCoder-Python-34B-V1.0` from `transformers`...\n",
      "Loading pretrained config for `WizardLM/WizardCoder-Python-34B-V1.0` from `transformers`...\n",
      "Loading pretrained config for `WizardLM/WizardCoder-Python-34B-V1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 239/500 [35:05<25:38,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model m42-health/med42-70b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 650/650 [00:00<00:00, 377kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 965/965 [00:00<00:00, 228kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 9.83MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.43MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 19.8kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 95.0/95.0 [00:00<00:00, 30.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `oobabooga/CodeBooga-34B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `oobabooga/CodeBooga-34B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `oobabooga/CodeBooga-34B-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 769/769 [00:00<00:00, 629kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.00k/1.00k [00:00<00:00, 654kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.30MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 509/509 [00:00<00:00, 228kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `macadeliccc/laser-dolphin-mixtral-2x7b-dpo` from `transformers`...\n",
      "Loading pretrained config for `macadeliccc/laser-dolphin-mixtral-2x7b-dpo` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 663/663 [00:00<00:00, 334kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 938kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 11.1MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 12.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 7.51MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-14B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-14B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 732/732 [00:00<00:00, 161kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 19.2k/19.2k [00:00<00:00, 14.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.35M/1.35M [00:00<00:00, 11.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 176kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `croissantllm/CroissantLLMChat-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 670/670 [00:00<00:00, 171kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.48k/1.48k [00:00<00:00, 1.16MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 1.76MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 624/624 [00:00<00:00, 261kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ZySec-AI/ZySec-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 4.29k/4.29k [00:00<00:00, 1.89MB/s]\n",
      "tokenizer.json: 100%|██████████| 4.61M/4.61M [00:00<00:00, 13.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 482/482 [00:00<00:00, 141kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `chatdb/natural-sql-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 2.20MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.28MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 310kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `occiglot/occiglot-7b-eu5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.12k/1.12k [00:00<00:00, 1.16MB/s]\n",
      " 49%|████▉     | 247/500 [35:56<18:14,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model amazon/chronos-t5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 706/706 [00:00<00:00, 198kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.71k/1.71k [00:00<00:00, 2.56MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 7.27MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 13.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 900/900 [00:00<00:00, 2.29MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `dreamgen/opus-v1-34b` from `transformers`...\n",
      "Loading pretrained config for `dreamgen/opus-v1-34b` from `transformers`...\n",
      "Loading pretrained config for `dreamgen/opus-v1-34b` from `transformers`...\n",
      "Loading pretrained config for `dreamgen/opus-v1-34b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 704/704 [00:00<00:00, 131kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.86k/1.86k [00:00<00:00, 9.77MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.18M/2.18M [00:00<00:00, 14.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 997kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `yanolja/EEVE-Korean-Instruct-10.8B-v1.0` from `transformers`...\n",
      "Loading pretrained config for `yanolja/EEVE-Korean-Instruct-10.8B-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 667/667 [00:00<00:00, 1.20MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 2.36MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 6.59MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 12.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 555/555 [00:00<00:00, 1.14MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mlabonne/Gemmalpaca-2B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 762/762 [00:00<00:00, 1.47MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 918/918 [00:00<00:00, 810kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 20.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `MBZUAI/MobiLlama-1B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 763/763 [00:00<00:00, 826kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.10k/1.10k [00:00<00:00, 2.08MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.7MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 55.3kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 169/169 [00:00<00:00, 147kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `MBZUAI/MobiLlama-05B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 707/707 [00:00<00:00, 1.54MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.94k/1.94k [00:00<00:00, 467kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 15.2MB/s]\n",
      "added_tokens.json: 100%|██████████| 144/144 [00:00<00:00, 258kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 197/197 [00:00<00:00, 298kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TIGER-Lab/StructLM-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 981/981 [00:00<00:00, 272kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 8.02k/8.02k [00:00<00:00, 11.8MB/s]\n",
      "vocab.json: 100%|██████████| 974k/974k [00:00<00:00, 14.3MB/s]\n",
      "merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 14.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.33k/1.33k [00:00<00:00, 1.76MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-SC2-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 709/709 [00:00<00:00, 710kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.98k/1.98k [00:00<00:00, 2.65MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 824kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `occiglot/occiglot-7b-de-en-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 669/669 [00:00<00:00, 212kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 1.25MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 200kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cloudyu/mistral_pretrain_demo` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 703/703 [00:00<00:00, 149kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 394kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 42.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 13.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 548/548 [00:00<00:00, 199kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sophosympatheia/Midnight-Miqu-70B-v1.5` from `transformers`...\n",
      "Loading pretrained config for `sophosympatheia/Midnight-Miqu-70B-v1.5` from `transformers`...\n",
      "Loading pretrained config for `sophosympatheia/Midnight-Miqu-70B-v1.5` from `transformers`...\n",
      "Loading pretrained config for `sophosympatheia/Midnight-Miqu-70B-v1.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 644/644 [00:00<00:00, 214kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.00k/1.00k [00:00<00:00, 788kB/s]\n",
      "tokenizer.model: 100%|██████████| 755k/755k [00:00<00:00, 7.61MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.61M/2.61M [00:00<00:00, 4.28MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 33.5kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 48.3kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Rakuten/RakutenAI-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 685/685 [00:00<00:00, 286kB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 13.0MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.10MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 441/441 [00:00<00:00, 884kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `facebook/opt-2.7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 879kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 9.80MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 9.06MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 1.26MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Llama-2-7B-Chat-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 709/709 [00:00<00:00, 615kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 942/942 [00:00<00:00, 2.08MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 4.00/4.00 [00:00<00:00, 15.6kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 485kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `togethercomputer/LLaMA-2-7B-32K` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 264/500 [37:36<10:49,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model SnypzZz/Llama2-13b-Language-translate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 912/912 [00:00<00:00, 305kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 4.73MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 487/487 [00:00<00:00, 198kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `alpindale/goliath-120b` from `transformers`...\n",
      "Loading pretrained config for `alpindale/goliath-120b` from `transformers`...\n",
      "Loading pretrained config for `alpindale/goliath-120b` from `transformers`...\n",
      "Loading pretrained config for `alpindale/goliath-120b` from `transformers`...\n",
      "Loading pretrained config for `alpindale/goliath-120b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 4.87k/4.87k [00:00<00:00, 1.69MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 2.70MB/s]\n",
      "added_tokens.json: 100%|██████████| 458/458 [00:00<00:00, 1.90MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 482/482 [00:00<00:00, 2.25MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ise-uiuc/Magicoder-S-DS-6.7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.70k/1.70k [00:00<00:00, 1.02MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 253kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 443/443 [00:00<00:00, 2.37MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/dolphin-2.5-mixtral-8x7b-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/dolphin-2.5-mixtral-8x7b-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 946kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.68MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `SanjiWatsuki/Silicon-Maid-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 6.61MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 8.52MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 1.95MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `segolilylabs/Lily-Cybersecurity-7B-v0.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 1.55MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.34MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 2.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `unsloth/mistral-7b-instruct-v0.2-bnb-4bit` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.47k/1.47k [00:00<00:00, 2.36MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 12.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 5.23MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 2.26MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Smaug-34B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-34B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-34B-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-34B-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.29k/1.29k [00:00<00:00, 296kB/s]\n",
      "tokenizer.model: 100%|██████████| 780k/780k [00:00<00:00, 2.75MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.66M/2.66M [00:00<00:00, 14.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 2.01MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `SeaLLMs/SeaLLM-7B-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 933/933 [00:00<00:00, 713kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.65MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 4.69MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 548/548 [00:00<00:00, 1.61MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `alchemonaut/QuartetAnemoi-70B-t0.0001` from `transformers`...\n",
      "Loading pretrained config for `alchemonaut/QuartetAnemoi-70B-t0.0001` from `transformers`...\n",
      "Loading pretrained config for `alchemonaut/QuartetAnemoi-70B-t0.0001` from `transformers`...\n",
      "Loading pretrained config for `alchemonaut/QuartetAnemoi-70B-t0.0001` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 706/706 [00:00<00:00, 264kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 918/918 [00:00<00:00, 508kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 348kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `USAIL-HKUSTGZ/LLMLight-LightGPT` from `transformers`...\n",
      "Loading pretrained config for `USAIL-HKUSTGZ/LLMLight-LightGPT` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 666/666 [00:00<00:00, 205kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.92k/1.92k [00:00<00:00, 609kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 87.0/87.0 [00:00<00:00, 22.9kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 535/535 [00:00<00:00, 195kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-CL-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-CL-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-CL-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 773/773 [00:00<00:00, 161kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 409kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 673kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.29MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 268kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-metaoffload-HQQ` from `transformers`...\n",
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-metaoffload-HQQ` from `transformers`...\n",
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-metaoffload-HQQ` from `transformers`...\n",
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bit-metaoffload-HQQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 701/701 [00:00<00:00, 432kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.92k/1.92k [00:00<00:00, 4.18MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 87.0/87.0 [00:00<00:00, 206kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 538/538 [00:00<00:00, 1.12MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TIGER-Lab/StructLM-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TIGER-Lab/StructLM-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TIGER-Lab/StructLM-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.14k/1.14k [00:00<00:00, 1.20MB/s]\n",
      " 56%|█████▌    | 281/500 [39:55<35:28,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model amazon/chronos-t5-tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 654/654 [00:00<00:00, 1.01MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.04k/2.04k [00:00<00:00, 9.09MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.3MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 253kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 558/558 [00:00<00:00, 2.92MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.8-experiment26-7b-preview` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 644/644 [00:00<00:00, 210kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.34k/1.34k [00:00<00:00, 1.82MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 975kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `MTSAIR/multi_verse_model` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 698/698 [00:00<00:00, 701kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.28k/1.28k [00:00<00:00, 5.04MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 4.49MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 13.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 13.4MB/s]\n",
      "added_tokens.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 292kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 370/370 [00:00<00:00, 1.11MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `M4-ai/tau-0.5B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 649/649 [00:00<00:00, 211kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.00k/1.00k [00:00<00:00, 823kB/s]\n",
      "tokenizer.model: 100%|██████████| 755k/755k [00:00<00:00, 13.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.61M/2.61M [00:00<00:00, 4.38MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 17.0kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 56.7kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Rakuten/RakutenAI-7B-chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 238/238 [00:00<00:00, 175kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 3.92MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 56.1kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mosaicml/mpt-7b-storywriter` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 787/787 [00:00<00:00, 250kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.50k/2.50k [00:00<00:00, 3.29MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 20.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 13.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 605kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n",
      "Loading pretrained config for `grammarly/coedit-large` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 175/175 [00:00<00:00, 432kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.34MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 3.18kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `AdaptLLM/finance-LLM` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 619/619 [00:00<00:00, 3.07MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 996/996 [00:00<00:00, 2.21MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 1.24MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 95.0/95.0 [00:00<00:00, 141kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `teknium/Mistral-Trismegistus-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.52k/1.52k [00:00<00:00, 857kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.31MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 210/210 [00:00<00:00, 621kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `amazon/MistralLite` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 4.66k/4.66k [00:00<00:00, 3.10MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 966/966 [00:00<00:00, 1.01MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 48.9kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `KoboldAI/LLaMA2-13B-Tiefighter` from `transformers`...\n",
      "Loading pretrained config for `KoboldAI/LLaMA2-13B-Tiefighter` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 608/608 [00:00<00:00, 467kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 322/322 [00:00<00:00, 1.29MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 13.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 13.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `01-ai/Yi-6B-200K` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 625/625 [00:00<00:00, 88.2kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 953/953 [00:00<00:00, 1.58MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.32MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 145/145 [00:00<00:00, 116kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Intel/neural-chat-7b-v3-1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 849/849 [00:00<00:00, 682kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 16.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 13.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 781kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `AdaptLLM/finance-chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.04k/1.04k [00:00<00:00, 359kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 1.47MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.40MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 280kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ` from `transformers`...\n",
      "Loading pretrained config for `TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 299/500 [41:21<11:38,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model Trelis/Mixtral-8x7B-Instruct-v0.1-function-calling-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 676k/676k [00:00<00:00, 2.76MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 72.2kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 563/563 [00:00<00:00, 1.33MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 743kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 10.9MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 12.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 12.9MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-1.8B-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 302/500 [41:30<10:10,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model INX-TEXT/Bailong-instruct-7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 667/667 [00:00<00:00, 244kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.89k/1.89k [00:00<00:00, 4.25MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 12.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 630/630 [00:00<00:00, 192kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `HuggingFaceH4/zephyr-7b-gemma-sft-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 655/655 [00:00<00:00, 240kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.42k/1.42k [00:00<00:00, 119kB/s]\n",
      "tokenizer.model: 100%|██████████| 712k/712k [00:00<00:00, 11.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 147kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Trendyol/Trendyol-LLM-7b-chat-dpo-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 683/683 [00:00<00:00, 812kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.07k/2.07k [00:00<00:00, 4.95MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.2MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 9.53kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 258kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `l3utterfly/mistral-7b-v0.1-layla-v4-chatml` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 1.60MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 3.23MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 13.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 13.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 4.22MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-small` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 818/818 [00:00<00:00, 3.77MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 449/449 [00:00<00:00, 766kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 12.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 228/228 [00:00<00:00, 89.8kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `databricks/dolly-v2-12b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `databricks/dolly-v2-12b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 810/810 [00:00<00:00, 4.13MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 1.48MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.38MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 1.91MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 2.91MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 287/287 [00:00<00:00, 36.0kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.73M/2.73M [00:00<00:00, 11.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 281/281 [00:00<00:00, 46.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `tiiuae/falcon-40b-instruct` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-40b-instruct` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-40b-instruct` from `transformers`...\n",
      "Loading pretrained config for `tiiuae/falcon-40b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 646/646 [00:00<00:00, 1.27MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 695/695 [00:00<00:00, 2.97MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 20.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 758kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NousResearch/Nous-Hermes-Llama2-13b` from `transformers`...\n",
      "Loading pretrained config for `NousResearch/Nous-Hermes-Llama2-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 311/500 [42:24<20:49,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codellama/CodeLlama-7b-Python-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.59k/1.59k [00:00<00:00, 7.24MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 20.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 12.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 804kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codellama/CodeLlama-7b-Instruct-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.75k/1.75k [00:00<00:00, 7.60MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 904/904 [00:00<00:00, 3.77MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 21.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.42MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 95.0/95.0 [00:00<00:00, 109kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `PygmalionAI/mythalion-13b` from `transformers`...\n",
      "Loading pretrained config for `PygmalionAI/mythalion-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 315/500 [42:39<12:57,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model CausalLM/14B-DPO-alpha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 582/582 [00:00<00:00, 2.76MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 828/828 [00:00<00:00, 1.67MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 69.0/69.0 [00:00<00:00, 160kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 1.05MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `microsoft/Orca-2-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.08k/1.08k [00:00<00:00, 2.16MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 2.83MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 13.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 11.6kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Mistral-7B-Instruct-v0.2-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 595/595 [00:00<00:00, 314kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 915/915 [00:00<00:00, 2.64MB/s]\n",
      "tokenizer.model: 100%|██████████| 563k/563k [00:00<00:00, 14.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.99M/1.99M [00:00<00:00, 3.66MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 187kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `scb10x/typhoon-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 668/668 [00:00<00:00, 776kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.88k/1.88k [00:00<00:00, 2.86MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 9.81MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 297/297 [00:00<00:00, 275kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `semantixai/LloroV2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 746/746 [00:00<00:00, 2.26MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 11.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 372kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Wanfq/FuseLLM-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 663/663 [00:00<00:00, 1.63MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 585kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 4.51MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 3.24MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 13.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 295/295 [00:00<00:00, 521kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 635/635 [00:00<00:00, 329kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 777/777 [00:00<00:00, 3.20MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 5.73MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 17.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 985kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `h2oai/h2o-danube-1.8b-base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 767/767 [00:00<00:00, 721kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 968/968 [00:00<00:00, 1.70MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 16.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 14.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 436/436 [00:00<00:00, 888kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `nakodanei/Blue-Orchid-2x7b` from `transformers`...\n",
      "Loading pretrained config for `nakodanei/Blue-Orchid-2x7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 644/644 [00:00<00:00, 212kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 1.88MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 13.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 284kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `nvidia/OpenMath-Mistral-7B-v0.1-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 37.6k/37.6k [00:00<00:00, 10.6MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 14.4MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 13.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.07M/7.07M [00:00<00:00, 12.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 441/441 [00:00<00:00, 430kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ibivibiv/alpaca-dragon-72b-v1` from `transformers`...\n",
      "Loading pretrained config for `ibivibiv/alpaca-dragon-72b-v1` from `transformers`...\n",
      "Loading pretrained config for `ibivibiv/alpaca-dragon-72b-v1` from `transformers`...\n",
      "Loading pretrained config for `ibivibiv/alpaca-dragon-72b-v1` from `transformers`...\n",
      "Loading pretrained config for `ibivibiv/alpaca-dragon-72b-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 7.01k/7.01k [00:00<00:00, 2.60MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.11M/1.11M [00:00<00:00, 13.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.98M/3.98M [00:00<00:00, 5.62MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.06k/1.06k [00:00<00:00, 877kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 306kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Vikhrmodels/Vikhr-7B-instruct_0.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 705/705 [00:00<00:00, 582kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 937/937 [00:00<00:00, 715kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 331kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `yaofu/llama-2-7b-80k` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 805/805 [00:00<00:00, 254kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.51k/1.51k [00:00<00:00, 526kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 327kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Smaug-Mixtral-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-Mixtral-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-Mixtral-v0.1` from `transformers`...\n",
      "Loading pretrained config for `abacusai/Smaug-Mixtral-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 695/695 [00:00<00:00, 839kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 8.24k/8.24k [00:00<00:00, 6.44MB/s]\n",
      " 66%|██████▌   | 330/500 [44:51<41:50, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model p1atdev/dart-v1-sft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 927/927 [00:00<00:00, 1.51MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.89k/1.89k [00:00<00:00, 4.45MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.57M/2.57M [00:00<00:00, 12.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 565/565 [00:00<00:00, 1.25MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `yanolja/EEVE-Korean-Instruct-2.8B-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 623/623 [00:00<00:00, 1.10MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 2.40MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 10.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.36MB/s]\n",
      "added_tokens.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 214kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 491/491 [00:00<00:00, 2.02MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `FuseAI/OpenChat-3.5-7B-Mixtral` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 687/687 [00:00<00:00, 2.72MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 1.89MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 13.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 13.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 555/555 [00:00<00:00, 2.78MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `VAGOsolutions/SauerkrautLM-Gemma-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 728/728 [00:00<00:00, 247kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 455kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 1.68MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 182kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Severian/Nexus-IKM-Mistral-7B-Pytorch` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 694/694 [00:00<00:00, 163kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 1.89MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 11.9MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 12.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 13.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 166kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 367/367 [00:00<00:00, 1.71MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Liberated-Qwen1.5-14B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/Liberated-Qwen1.5-14B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 651/651 [00:00<00:00, 447kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.45k/1.45k [00:00<00:00, 1.22MB/s]\n",
      "tokenizer.model: 100%|██████████| 712k/712k [00:00<00:00, 13.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 83.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Trendyol/Trendyol-LLM-7b-base-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 689/689 [00:00<00:00, 857kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 909/909 [00:00<00:00, 2.02MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 8.55MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 13.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 573/573 [00:00<00:00, 2.99MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abacusai/bigyi-15b` from `transformers`...\n",
      "Loading pretrained config for `abacusai/bigyi-15b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 642/642 [00:00<00:00, 1.32MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 2.34MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 1.80MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 13.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 181kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Endevor/InfinityRP-v1-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 718/718 [00:00<00:00, 494kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 4.20k/4.20k [00:00<00:00, 12.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 12.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 482/482 [00:00<00:00, 2.18MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `arise-sustech/llm4decompile-6.7b-uo` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 613/613 [00:00<00:00, 1.35MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 156/156 [00:00<00:00, 282kB/s]\n",
      "vocab.json: 100%|██████████| 1.08M/1.08M [00:00<00:00, 2.47MB/s]\n",
      "merges.txt: 100%|██████████| 457k/457k [00:00<00:00, 11.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 11.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 343kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `EleutherAI/gpt-neox-20b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `EleutherAI/gpt-neox-20b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `EleutherAI/gpt-neox-20b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 848/848 [00:00<00:00, 3.62MB/s]\n",
      "vocab.json: 100%|██████████| 999k/999k [00:00<00:00, 13.2MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 14.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 548/548 [00:00<00:00, 2.11MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `KoboldAI/OPT-13B-Erebus` from `transformers`...\n",
      "Loading pretrained config for `KoboldAI/OPT-13B-Erebus` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.44k/1.44k [00:00<00:00, 2.94MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 6.51MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 13.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 5.35MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 4.32MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n",
      "Loading pretrained config for `google/flan-t5-xl` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 784/784 [00:00<00:00, 1.75MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.35k/2.35k [00:00<00:00, 4.97MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 12.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.43M/2.43M [00:00<00:00, 11.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 665/665 [00:00<00:00, 3.32MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 3.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `google/flan-ul2` from `transformers`...\n",
      "Loading pretrained config for `google/flan-ul2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 819/819 [00:00<00:00, 1.41MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 450/450 [00:00<00:00, 471kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 11.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 228/228 [00:00<00:00, 341kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `databricks/dolly-v2-3b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.23k/1.23k [00:00<00:00, 3.47MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 237/237 [00:00<00:00, 1.16MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 3.84MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 516kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mosaicml/mpt-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 621/621 [00:00<00:00, 2.49MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 1.76MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.1MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 99.1kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 2.13MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Gryphe/MythoMax-L2-13b` from `transformers`...\n",
      "Loading pretrained config for `Gryphe/MythoMax-L2-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 639/639 [00:00<00:00, 1.44MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 855/855 [00:00<00:00, 4.05MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 10.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.93MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `princeton-nlp/SWE-Llama-13b` from `transformers`...\n",
      "Loading pretrained config for `princeton-nlp/SWE-Llama-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 622/622 [00:00<00:00, 2.92MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.69k/1.69k [00:00<00:00, 3.72MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 238kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 100/100 [00:00<00:00, 248kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.1-mistral-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 607/607 [00:00<00:00, 764kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 855/855 [00:00<00:00, 2.58MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 11.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.10MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `kaist-ai/prometheus-13b-v1.0` from `transformers`...\n",
      "Loading pretrained config for `kaist-ai/prometheus-13b-v1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 355/500 [46:51<09:41,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model CausalLM/7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 632/632 [00:00<00:00, 2.58MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 793/793 [00:00<00:00, 1.64MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 2.73MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `deepseek-ai/deepseek-coder-6.7b-base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 605/605 [00:00<00:00, 1.13MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 320/320 [00:00<00:00, 726kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 13.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 12.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `01-ai/Yi-6B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.15k/1.15k [00:00<00:00, 3.63MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 953/953 [00:00<00:00, 4.26MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 13.1MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 194kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 145/145 [00:00<00:00, 597kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Yarn-Mistral-7B-128k-AWQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 985/985 [00:00<00:00, 829kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.85M/1.85M [00:00<00:00, 3.41MB/s]\n",
      "added_tokens.json: 100%|██████████| 195/195 [00:00<00:00, 137kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 623/623 [00:00<00:00, 434kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Nexusflow/NexusRaven-V2-13B` from `transformers`...\n",
      "Loading pretrained config for `Nexusflow/NexusRaven-V2-13B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 609/609 [00:00<00:00, 449kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 953/953 [00:00<00:00, 1.78MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 135kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 145/145 [00:00<00:00, 202kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Intel/neural-chat-7b-v3-3` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 904/904 [00:00<00:00, 1.19MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 2.43MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 120kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Mistral-7B-Instruct-v0.2-AWQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 613/613 [00:00<00:00, 898kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 953/953 [00:00<00:00, 827kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.35MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 487/487 [00:00<00:00, 2.45MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `rishiraj/CatPPT-base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 1.98MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.38MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.05MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `casperhansen/mixtral-instruct-awq` from `transformers`...\n",
      "Loading pretrained config for `casperhansen/mixtral-instruct-awq` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 642/642 [00:00<00:00, 1.03MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 2.15MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 896kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `OpenPipe/mistral-ft-optimized-1227` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 687/687 [00:00<00:00, 400kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 4.46k/4.46k [00:00<00:00, 3.49MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 12.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 625/625 [00:00<00:00, 851kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WizardLM/WizardCoder-33B-V1.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WizardLM/WizardCoder-33B-V1.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WizardLM/WizardCoder-33B-V1.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 616/616 [00:00<00:00, 571kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.79k/2.79k [00:00<00:00, 13.0MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 95.0/95.0 [00:00<00:00, 440kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 4.61MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meetkai/functionary-small-v2.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 716/716 [00:00<00:00, 963kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 18.6k/18.6k [00:00<00:00, 28.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.35M/1.35M [00:00<00:00, 11.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.54MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `croissantllm/CroissantLLMBase` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 654/654 [00:00<00:00, 2.12MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.69k/1.69k [00:00<00:00, 7.37MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.4MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 202kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 416/416 [00:00<00:00, 1.82MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `DiscoResearch/DiscoLM_German_7b_v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 2.73k/2.73k [00:00<00:00, 5.37MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 4.14MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 10.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 211kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 1.75MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 484kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 303kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf` from `transformers`...\n",
      "Loading pretrained config for `ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.35k/1.35k [00:00<00:00, 1.70MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 4.86MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 436/436 [00:00<00:00, 1.69MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `GritLM/GritLM-8x7B` from `transformers`...\n",
      "Loading pretrained config for `GritLM/GritLM-8x7B` from `transformers`...\n",
      "Loading pretrained config for `GritLM/GritLM-8x7B` from `transformers`...\n",
      "Loading pretrained config for `GritLM/GritLM-8x7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 578/578 [00:00<00:00, 459kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.53M/2.53M [00:00<00:00, 4.17MB/s]\n",
      "tokenizer.json: 100%|██████████| 5.17M/5.17M [00:00<00:00, 6.21MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 128kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `beomi/SOLAR-KOEN-10.8B` from `transformers`...\n",
      "Loading pretrained config for `beomi/SOLAR-KOEN-10.8B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 666/666 [00:00<00:00, 449kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.92k/1.92k [00:00<00:00, 4.10MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 535/535 [00:00<00:00, 1.08MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/OpenCodeInterpreter-CL-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 631/631 [00:00<00:00, 598kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 968/968 [00:00<00:00, 5.06MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 1.88MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `PleIAs/OCRonos` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 638kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.16k/2.16k [00:00<00:00, 3.82MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 13.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 13.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 528kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `unsloth/gemma-7b-it-bnb-4bit` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 669/669 [00:00<00:00, 625kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.15k/2.15k [00:00<00:00, 7.06MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 10.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 10.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 3.00MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abideen/gemma-7b-openhermes` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 683/683 [00:00<00:00, 352kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 935/935 [00:00<00:00, 645kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 5.26MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 2.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `OPI-PG/Qra-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 728/728 [00:00<00:00, 756kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.12k/1.12k [00:00<00:00, 1.52MB/s]\n",
      "tokenizer.model: 100%|██████████| 512k/512k [00:00<00:00, 11.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.87M/1.87M [00:00<00:00, 5.66MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 1.63MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `piotr-ai/polanka-3b-pretrain-full-v0.4` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 381/500 [51:09<10:13,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model webbigdata/C3TR-Adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 692/692 [00:00<00:00, 3.14MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.42k/1.42k [00:00<00:00, 5.45MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 4.63MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 2.39MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cmcmaster/il_7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.13k/1.13k [00:00<00:00, 456kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.01k/1.01k [00:00<00:00, 2.07MB/s]\n",
      "vocab.json: 100%|██████████| 927k/927k [00:00<00:00, 12.3MB/s]\n",
      "merges.txt: 100%|██████████| 585k/585k [00:00<00:00, 15.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.37M/2.37M [00:00<00:00, 11.9MB/s]\n",
      "added_tokens.json: 100%|██████████| 24.0/24.0 [00:00<00:00, 46.1kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 352/352 [00:00<00:00, 726kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 730/730 [00:00<00:00, 2.49MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 375/375 [00:00<00:00, 800kB/s]\n",
      "spiece.model: 100%|██████████| 4.31M/4.31M [00:00<00:00, 11.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 65.0/65.0 [00:00<00:00, 123kB/s]\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/hf/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `csebuetnlp/mT5_multilingual_XLSum` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 738/738 [00:00<00:00, 1.67MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 7.44MB/s]\n",
      "vocab.json: 100%|██████████| 1.89M/1.89M [00:00<00:00, 3.49MB/s]\n",
      "merges.txt: 100%|██████████| 1.20M/1.20M [00:00<00:00, 11.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 606/606 [00:00<00:00, 2.08MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ai-forever/mGPT` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 693/693 [00:00<00:00, 1.01MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 222/222 [00:00<00:00, 558kB/s]\n",
      "tokenizer.json: 100%|██████████| 14.5M/14.5M [00:01<00:00, 11.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 185kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `bigscience/bloom-560m` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 258kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 12.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 351kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Llama-2-13B-chat-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 638/638 [00:00<00:00, 517kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 225kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 137kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `lmsys/vicuna-13b-v1.5` from `transformers`...\n",
      "Loading pretrained config for `lmsys/vicuna-13b-v1.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 678/678 [00:00<00:00, 216kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 743/743 [00:00<00:00, 228kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.34MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 9.95MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 213kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `PharMolix/BioMedGPT-LM-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.01k/1.01k [00:00<00:00, 775kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 717/717 [00:00<00:00, 601kB/s]\n",
      "vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 11.4MB/s]\n",
      "merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 8.80MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 11.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 532/532 [00:00<00:00, 403kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `defog/sqlcoder` from `transformers`...\n",
      "Loading pretrained config for `defog/sqlcoder` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 791/791 [00:00<00:00, 490kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 828/828 [00:00<00:00, 644kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 170kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `FreedomIntelligence/AceGPT-13B-chat` from `transformers`...\n",
      "Loading pretrained config for `FreedomIntelligence/AceGPT-13B-chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 645/645 [00:00<00:00, 491kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 824/824 [00:00<00:00, 504kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.34MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 157kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ToolBench/ToolLLaMA-2-7b-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.64k/1.64k [00:00<00:00, 1.25MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 20.9MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 8.42kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 174/174 [00:00<00:00, 73.7kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `teknium/OpenHermes-2-Mistral-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 659/659 [00:00<00:00, 419kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 700/700 [00:00<00:00, 910kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 11.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 865kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `maritaca-ai/sabia-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 690/690 [00:00<00:00, 627kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 893/893 [00:00<00:00, 1.74MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 4.10MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 4.72MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 86.5kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 145/145 [00:00<00:00, 274kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `jebcarter/psyonic-cetacean-20B` from `transformers`...\n",
      "Loading pretrained config for `jebcarter/psyonic-cetacean-20B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 398/500 [52:28<06:56,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model ura-hcmut/MixSUra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 763/763 [00:00<00:00, 742kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.22k/1.22k [00:00<00:00, 4.10MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.26MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 509/509 [00:00<00:00, 2.23MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mlabonne/Beyonder-4x7B-v2` from `transformers`...\n",
      "Loading pretrained config for `mlabonne/Beyonder-4x7B-v2` from `transformers`...\n",
      "Loading pretrained config for `mlabonne/Beyonder-4x7B-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 868/868 [00:00<00:00, 1.27MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.41k/1.41k [00:00<00:00, 5.56MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 12.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 11.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 436/436 [00:00<00:00, 738kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cloudyu/Yi-34Bx2-MoE-60B` from `transformers`...\n",
      "Loading pretrained config for `cloudyu/Yi-34Bx2-MoE-60B` from `transformers`...\n",
      "Loading pretrained config for `cloudyu/Yi-34Bx2-MoE-60B` from `transformers`...\n",
      "Loading pretrained config for `cloudyu/Yi-34Bx2-MoE-60B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 151kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.60k/1.60k [00:00<00:00, 2.74MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.21MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 62.0kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 993kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-AWQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Nous-Hermes-2-Mixtral-8x7B-DPO-AWQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 662/662 [00:00<00:00, 828kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 1.22MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 12.4MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 12.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 7.45MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Qwen/Qwen1.5-4B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 706/706 [00:00<00:00, 282kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 869/869 [00:00<00:00, 140kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 16.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 14.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 143kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `KatyTheCutie/EstopianMaid-13B` from `transformers`...\n",
      "Loading pretrained config for `KatyTheCutie/EstopianMaid-13B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 635/635 [00:00<00:00, 215kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 932/932 [00:00<00:00, 674kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 18.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.29MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 197kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codellama/CodeLlama-70b-Python-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-70b-Python-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-70b-Python-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-70b-Python-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 635/635 [00:00<00:00, 267kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.66k/1.66k [00:00<00:00, 529kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.39MB/s]\n",
      "added_tokens.json: 100%|██████████| 22.0/22.0 [00:00<00:00, 10.5kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 140kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codellama/CodeLlama-70b-Instruct-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-70b-Instruct-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-70b-Instruct-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-70b-Instruct-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 636/636 [00:00<00:00, 290kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 14.5k/14.5k [00:00<00:00, 3.37MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 19.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.72k/1.72k [00:00<00:00, 555kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.98k/1.98k [00:00<00:00, 822kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `jondurbin/bagel-dpo-7b-v0.4` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 591/591 [00:00<00:00, 335kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 111/111 [00:00<00:00, 197kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.38M/1.38M [00:00<00:00, 2.69MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 106/106 [00:00<00:00, 173kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `norallm/normistral-7b-warm` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 594/594 [00:00<00:00, 827kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.14k/1.14k [00:00<00:00, 1.42MB/s]\n",
      "tokenizer.json: 100%|██████████| 4.61M/4.61M [00:00<00:00, 5.88MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `deepseek-ai/deepseek-math-7b-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 674/674 [00:00<00:00, 253kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.17k/1.17k [00:00<00:00, 572kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 11.2MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 11.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 12.2MB/s]\n",
      "added_tokens.json: 100%|██████████| 80.0/80.0 [00:00<00:00, 24.5kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 510/510 [00:00<00:00, 418kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `vilm/Quyen-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 621/621 [00:00<00:00, 418kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.70k/1.70k [00:00<00:00, 853kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.9MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 18.9kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 82.6kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `kubernetes-bad/chargen-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 714/714 [00:00<00:00, 583kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.48k/1.48k [00:00<00:00, 3.07MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 606kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `NeverSleep/MiquMaid-v2-70B-DPO` from `transformers`...\n",
      "Loading pretrained config for `NeverSleep/MiquMaid-v2-70B-DPO` from `transformers`...\n",
      "Loading pretrained config for `NeverSleep/MiquMaid-v2-70B-DPO` from `transformers`...\n",
      "Loading pretrained config for `NeverSleep/MiquMaid-v2-70B-DPO` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 690/690 [00:00<00:00, 323kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 7.90k/7.90k [00:00<00:00, 4.06MB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 14.2MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 15.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.12M/2.12M [00:00<00:00, 6.84MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.10k/1.10k [00:00<00:00, 747kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 579/579 [00:00<00:00, 495kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mobiuslabsgmbh/aanaphi2-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 678/678 [00:00<00:00, 543kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.43k/2.43k [00:00<00:00, 2.00MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.80MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.44MB/s]\n",
      "added_tokens.json: 100%|██████████| 142/142 [00:00<00:00, 96.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 974/974 [00:00<00:00, 806kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Unbabel/TowerInstruct-7B-v0.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 693/693 [00:00<00:00, 249kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 966/966 [00:00<00:00, 283kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 25.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 137kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Himitsui/Kaiju-11B` from `transformers`...\n",
      "Loading pretrained config for `Himitsui/Kaiju-11B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 6.15k/6.15k [00:00<00:00, 3.76MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 417kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 52.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ISTA-DASLab/Mixtral-8x7B-Instruct-v0_1-AQLM-2Bit-1x16-hf` from `transformers`...\n",
      "Loading pretrained config for `ISTA-DASLab/Mixtral-8x7B-Instruct-v0_1-AQLM-2Bit-1x16-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 642/642 [00:00<00:00, 534kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 969/969 [00:00<00:00, 1.89MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 2.83MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 9.80MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 71.8kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Locutusque/Hercules-3.1-Mistral-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 641/641 [00:00<00:00, 151kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 971/971 [00:00<00:00, 804kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 25.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 372kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `bardsai/jaskier-7b-dpo-v6.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 646/646 [00:00<00:00, 212kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.25k/1.25k [00:00<00:00, 152kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 624/624 [00:00<00:00, 1.91MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `abideen/AlphaMonarch-laser` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 441kB/s]\n",
      " 84%|████████▍ | 420/500 [55:34<06:40,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model amazon/chronos-t5-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 1.78MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.16k/2.16k [00:00<00:00, 7.95MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 13.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 12.6MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 194kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `unsloth/gemma-7b-bnb-4bit` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 466kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.16k/2.16k [00:00<00:00, 8.28MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 13.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 12.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 1.37MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `unsloth/gemma-2b-it-bnb-4bit` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 670/670 [00:00<00:00, 125kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.16k/2.16k [00:00<00:00, 750kB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 12.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 13.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 218kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `unsloth/gemma-7b-it` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 724/724 [00:00<00:00, 147kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.80k/1.80k [00:00<00:00, 539kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 22.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.4MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 39.3kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 557/557 [00:00<00:00, 221kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/DolphinHermes-120b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/DolphinHermes-120b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/DolphinHermes-120b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/DolphinHermes-120b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/DolphinHermes-120b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 686/686 [00:00<00:00, 208kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 867/867 [00:00<00:00, 653kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 149kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `USAIL-HKUSTGZ/UrbanKGent-13b` from `transformers`...\n",
      "Loading pretrained config for `USAIL-HKUSTGZ/UrbanKGent-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 667/667 [00:00<00:00, 106kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.92k/1.92k [00:00<00:00, 557kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 535/535 [00:00<00:00, 146kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TIGER-Lab/StructLM-13B` from `transformers`...\n",
      "Loading pretrained config for `TIGER-Lab/StructLM-13B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 618/618 [00:00<00:00, 297kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 824/824 [00:00<00:00, 241kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.42MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 6.94kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 125kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `m-a-p/ChatMusician-Base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 637/637 [00:00<00:00, 282kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 675k/675k [00:00<00:00, 14.1MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 17.0MB/s]\n",
      "added_tokens.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 2.02kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 563/563 [00:00<00:00, 187kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `0dAI/0dAI-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 773/773 [00:00<00:00, 272kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 2.24MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.33MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 711kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ` from `transformers`...\n",
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ` from `transformers`...\n",
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ` from `transformers`...\n",
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-3bit-metaoffload-HQQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 640/640 [00:00<00:00, 660kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 3.02MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 1.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ajibawa-2023/OpenHermes-2.5-Code-290k-13B` from `transformers`...\n",
      "Loading pretrained config for `ajibawa-2023/OpenHermes-2.5-Code-290k-13B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 641/641 [00:00<00:00, 459kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 4.83MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.23MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 948kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `KatyTheCutie/LemonadeRP-4.5.3` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 642/642 [00:00<00:00, 340kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 969/969 [00:00<00:00, 1.80MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 9.56MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 872kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Locutusque/Hyperion-1.5-Mistral-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 651/651 [00:00<00:00, 496kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 2.57MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 1.01MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `hydra-project/ChatHercules-2.5-Mistral-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 764/764 [00:00<00:00, 469kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.67k/1.67k [00:00<00:00, 3.50MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 19.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 9.39MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 2.60MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `maywell/KoMultiGen-General` from `transformers`...\n",
      "Loading pretrained config for `maywell/KoMultiGen-General` from `transformers`...\n",
      "Loading pretrained config for `maywell/KoMultiGen-General` from `transformers`...\n",
      "Loading pretrained config for `maywell/KoMultiGen-General` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 653/653 [00:00<00:00, 638kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.00k/1.00k [00:00<00:00, 104kB/s]\n",
      "tokenizer.model: 100%|██████████| 755k/755k [00:00<00:00, 2.72MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.61M/2.61M [00:00<00:00, 4.09MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 53.2kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 272kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Rakuten/RakutenAI-7B-instruct` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.56k/1.56k [00:00<00:00, 838kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.92k/1.92k [00:00<00:00, 4.05MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 11.2MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 3.39MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flax-community/t5-recipe-generation` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 642/642 [00:00<00:00, 550kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 614/614 [00:00<00:00, 783kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.38MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.43MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `microsoft/DialoGPT-large` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 876/876 [00:00<00:00, 1.43MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 267/267 [00:00<00:00, 501kB/s]\n",
      "vocab.json: 100%|██████████| 602k/602k [00:00<00:00, 5.59MB/s]\n",
      "merges.txt: 100%|██████████| 276k/276k [00:00<00:00, 83.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.23M/1.23M [00:00<00:00, 10.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `stanford-crfm/BioMedLM` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 683/683 [00:00<00:00, 107kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 1.71MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.33MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 312kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `FlagAlpha/Llama2-Chinese-13b-Chat` from `transformers`...\n",
      "Loading pretrained config for `FlagAlpha/Llama2-Chinese-13b-Chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 635/635 [00:00<00:00, 1.13MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 1.62MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.32MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 34.6kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 775kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `WizardLM/WizardLM-70B-V1.0` from `transformers`...\n",
      "Loading pretrained config for `WizardLM/WizardLM-70B-V1.0` from `transformers`...\n",
      "Loading pretrained config for `WizardLM/WizardLM-70B-V1.0` from `transformers`...\n",
      "Loading pretrained config for `WizardLM/WizardLM-70B-V1.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 589/589 [00:00<00:00, 409kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 1.34MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 11.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.32MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 824kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `codellama/CodeLlama-13b-hf` from `transformers`...\n",
      "Loading pretrained config for `codellama/CodeLlama-13b-hf` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.74k/1.74k [00:00<00:00, 6.86MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 904/904 [00:00<00:00, 1.39MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 10.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.43MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 95.0/95.0 [00:00<00:00, 444kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `PygmalionAI/pygmalion-2-13b` from `transformers`...\n",
      "Loading pretrained config for `PygmalionAI/pygmalion-2-13b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 579/579 [00:00<00:00, 648kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 745/745 [00:00<00:00, 1.42MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.29MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `EleutherAI/llemma_7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 619/619 [00:00<00:00, 461kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.41k/1.41k [00:00<00:00, 368kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 9.96MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 145/145 [00:00<00:00, 55.6kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `jphme/em_german_leo_mistral` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.50k/1.50k [00:00<00:00, 1.16MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.37k/2.37k [00:00<00:00, 2.30MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 12.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 3.41MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 3.53MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n",
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 451/500 [59:06<04:07,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Falconsai/medical_summarization` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 626/626 [00:00<00:00, 342kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 1.49MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 11.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 87.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 163kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `flozi00/Mistral-7B-german-assistant-v4` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 792/792 [00:00<00:00, 257kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.37k/2.37k [00:00<00:00, 2.14MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 3.45MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.97M/2.97M [00:00<00:00, 11.0MB/s]\n",
      "added_tokens.json: 100%|██████████| 58.3k/58.3k [00:00<00:00, 76.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.38k/2.38k [00:00<00:00, 179kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `QizhiPei/biot5-base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 618/618 [00:00<00:00, 59.2kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.70k/1.70k [00:00<00:00, 612kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.8MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 118kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 443/443 [00:00<00:00, 1.11MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphin-2.2.1-mistral-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 669/669 [00:00<00:00, 348kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 678/678 [00:00<00:00, 1.35MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.27M/3.27M [00:00<00:00, 4.88MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 585/585 [00:00<00:00, 337kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cyberagent/calm2-7b-chat` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 657/657 [00:00<00:00, 765kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 935/935 [00:00<00:00, 1.65MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 1.90MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 552/552 [00:00<00:00, 69.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `gorilla-llm/gorilla-openfunctions-v1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 667/667 [00:00<00:00, 1.01MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 857/857 [00:00<00:00, 92.6kB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 18.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 13.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 117/117 [00:00<00:00, 204kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `SUSTech/SUS-Chat-34B` from `transformers`...\n",
      "Loading pretrained config for `SUSTech/SUS-Chat-34B` from `transformers`...\n",
      "Loading pretrained config for `SUSTech/SUS-Chat-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 904/904 [00:00<00:00, 650kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 106/106 [00:00<00:00, 317kB/s]\n",
      "tokenizer.json: 100%|██████████| 3.42M/3.42M [00:00<00:00, 13.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 131kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `riotu-lab/ArabianGPT-01B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 2.92MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 971/971 [00:00<00:00, 4.52MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 19.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 13.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 446kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `unsloth/mistral-7b-bnb-4bit` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 714/714 [00:00<00:00, 896kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.01k/1.01k [00:00<00:00, 1.83MB/s]\n",
      "tokenizer.model: 100%|██████████| 672k/672k [00:00<00:00, 16.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.36M/2.36M [00:00<00:00, 13.7MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 3.31kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 625/625 [00:00<00:00, 1.85MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Vikhrmodels/Vikhr-7b-0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 690/690 [00:00<00:00, 368kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 959/959 [00:00<00:00, 2.11MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 15.2MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.31MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 985kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `nsfwthrowitaway69/Venus-120b-v1.2` from `transformers`...\n",
      "Loading pretrained config for `nsfwthrowitaway69/Venus-120b-v1.2` from `transformers`...\n",
      "Loading pretrained config for `nsfwthrowitaway69/Venus-120b-v1.2` from `transformers`...\n",
      "Loading pretrained config for `nsfwthrowitaway69/Venus-120b-v1.2` from `transformers`...\n",
      "Loading pretrained config for `nsfwthrowitaway69/Venus-120b-v1.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 607/607 [00:00<00:00, 506kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 3.49MB/s]\n",
      "vocab.json: 100%|██████████| 999k/999k [00:00<00:00, 2.31MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 14.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 189/189 [00:00<00:00, 52.8kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ahxt/LiteLlama-460M-1T` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 772/772 [00:00<00:00, 1.92MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.84k/2.84k [00:00<00:00, 1.06MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 15.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.24MB/s]\n",
      "added_tokens.json: 100%|██████████| 95.0/95.0 [00:00<00:00, 159kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 2.82MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meetkai/functionary-medium-v2.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meetkai/functionary-medium-v2.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meetkai/functionary-medium-v2.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `meetkai/functionary-medium-v2.2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 719/719 [00:00<00:00, 362kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.37k/1.37k [00:00<00:00, 900kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 18.5MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 177kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 443/443 [00:00<00:00, 96.7kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/TinyDolphin-2.8-1.1b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 715/715 [00:00<00:00, 51.9kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 749/749 [00:00<00:00, 2.98MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 21.3MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 10.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `motherduckdb/DuckDB-NSQL-7B-v0.1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 641/641 [00:00<00:00, 1.19MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 1.85MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 12.7MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 10.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.45MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cfahlgren1/natural-functions` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.73k/1.73k [00:00<00:00, 1.56MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.17k/1.17k [00:00<00:00, 569kB/s]\n",
      " 94%|█████████▎| 468/500 [1:00:52<03:12,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model aisingapore/sea-lion-7b-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 589/589 [00:00<00:00, 1.69MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 318/318 [00:00<00:00, 1.40MB/s]\n",
      "tokenizer.json: 100%|██████████| 14.5M/14.5M [00:01<00:00, 12.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 96.0/96.0 [00:00<00:00, 79.7kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sambanovasystems/BLOOMChat-176B-v2` from `transformers`...\n",
      "Loading pretrained config for `sambanovasystems/BLOOMChat-176B-v2` from `transformers`...\n",
      "Loading pretrained config for `sambanovasystems/BLOOMChat-176B-v2` from `transformers`...\n",
      "Loading pretrained config for `sambanovasystems/BLOOMChat-176B-v2` from `transformers`...\n",
      "Loading pretrained config for `sambanovasystems/BLOOMChat-176B-v2` from `transformers`...\n",
      "Loading pretrained config for `sambanovasystems/BLOOMChat-176B-v2` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 704/704 [00:00<00:00, 390kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 933/933 [00:00<00:00, 449kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 10.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 3.33MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 548/548 [00:00<00:00, 354kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sophosympatheia/Midnight-Rose-70B-v2.0.3` from `transformers`...\n",
      "Loading pretrained config for `sophosympatheia/Midnight-Rose-70B-v2.0.3` from `transformers`...\n",
      "Loading pretrained config for `sophosympatheia/Midnight-Rose-70B-v2.0.3` from `transformers`...\n",
      "Loading pretrained config for `sophosympatheia/Midnight-Rose-70B-v2.0.3` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 960/960 [00:00<00:00, 769kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.28k/1.28k [00:00<00:00, 6.04MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.20MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 1.19MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Nitral-AI/Kunocchini-7b-128k-test` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 787/787 [00:00<00:00, 240kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.51k/1.51k [00:00<00:00, 1.28MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 4.98MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 10.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 448kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `BiMediX/BiMediX-Eng` from `transformers`...\n",
      "Loading pretrained config for `BiMediX/BiMediX-Eng` from `transformers`...\n",
      "Loading pretrained config for `BiMediX/BiMediX-Eng` from `transformers`...\n",
      "Loading pretrained config for `BiMediX/BiMediX-Eng` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 750kB/s]\n",
      " 95%|█████████▍| 474/500 [1:01:45<03:04,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing model amazon/chronos-t5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 809/809 [00:00<00:00, 2.55MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 20.6k/20.6k [00:00<00:00, 11.0MB/s]\n",
      "spiece.model: 100%|██████████| 1.47M/1.47M [00:00<00:00, 12.0MB/s]\n",
      "added_tokens.json: 100%|██████████| 2.59k/2.59k [00:00<00:00, 12.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 10.5MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `utrobinmv/t5_summary_en_ru_zh_base_2048` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 865kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.16k/2.16k [00:00<00:00, 7.53MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 13.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 12.0MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 1.34MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `unsloth/gemma-2b-bnb-4bit` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 668/668 [00:00<00:00, 287kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 898kB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 5.10MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 3.82MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 7.33MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Minami-su/Qwen1.5-7B-Chat_mistral` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 808/808 [00:00<00:00, 444kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.24k/2.24k [00:00<00:00, 4.89MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 9.28MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 10.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 636/636 [00:00<00:00, 352kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `weqweasdas/RM-Gemma-2B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 669/669 [00:00<00:00, 134kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 379kB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 11.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 12.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 555/555 [00:00<00:00, 260kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mlabonne/Gemmalpaca-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 623/623 [00:00<00:00, 227kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 734kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 3.99MB/s]\n",
      "added_tokens.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 207kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 491/491 [00:00<00:00, 1.05MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `FuseAI/OpenChat-3.5-7B-Solar` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 659/659 [00:00<00:00, 161kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.51k/1.51k [00:00<00:00, 1.61MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 437/437 [00:00<00:00, 874kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `MaziyarPanahi/Mistral-7B-Instruct-Aya-101` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 662/662 [00:00<00:00, 596kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 2.25MB/s]\n",
      "vocab.json: 100%|██████████| 2.78M/2.78M [00:00<00:00, 4.65MB/s]\n",
      "merges.txt: 100%|██████████| 1.67M/1.67M [00:00<00:00, 10.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 7.03M/7.03M [00:00<00:00, 8.10MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `sail/Sailor-0.5B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 773/773 [00:00<00:00, 275kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 591kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 14.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 132kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bitgs8-metaoffload-HQQ` from `transformers`...\n",
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bitgs8-metaoffload-HQQ` from `transformers`...\n",
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bitgs8-metaoffload-HQQ` from `transformers`...\n",
      "Loading pretrained config for `mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-attn-4bit-moe-2bitgs8-metaoffload-HQQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 651/651 [00:00<00:00, 1.02MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.16k/1.16k [00:00<00:00, 2.03MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 1.79MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 551/551 [00:00<00:00, 1.19MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `hydra-project/OpenHercules-2.5-Mistral-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 631/631 [00:00<00:00, 1.12MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.06k/1.06k [00:00<00:00, 2.33MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 2.53MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.9MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 487/487 [00:00<00:00, 2.24MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ChaoticNeutrals/Eris_Remix_7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 950/950 [00:00<00:00, 354kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 8.92k/8.92k [00:00<00:00, 14.6MB/s]\n",
      "vocab.json: 100%|██████████| 777k/777k [00:00<00:00, 12.9MB/s]\n",
      "merges.txt: 100%|██████████| 442k/442k [00:00<00:00, 12.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.06M/2.06M [00:00<00:00, 13.0MB/s]\n",
      "added_tokens.json: 100%|██████████| 51.0/51.0 [00:00<00:00, 204kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.44k/1.44k [00:00<00:00, 2.76MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/dolphincoder-starcoder2-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 638/638 [00:00<00:00, 976kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 410kB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 17.5MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.03MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `ChaoticNeutrals/BuRP_7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 645/645 [00:00<00:00, 907kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 967/967 [00:00<00:00, 1.76MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 13.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:01<00:00, 1.33MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 454kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `grimjim/kukulemon-7B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 713/713 [00:00<00:00, 934kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 1.13MB/s]\n",
      "tokenizer.model: 100%|██████████| 1.03M/1.03M [00:00<00:00, 12.6MB/s]\n",
      "tokenizer.json: 100%|██████████| 3.56M/3.56M [00:00<00:00, 5.22MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 467/467 [00:00<00:00, 809kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Locutusque/Hyperion-3.0-Yi-34B` from `transformers`...\n",
      "Loading pretrained config for `Locutusque/Hyperion-3.0-Yi-34B` from `transformers`...\n",
      "Loading pretrained config for `Locutusque/Hyperion-3.0-Yi-34B` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 659/659 [00:00<00:00, 2.79MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.11k/1.11k [00:00<00:00, 2.28MB/s]\n",
      "tokenizer.model: 100%|██████████| 4.24M/4.24M [00:00<00:00, 7.54MB/s]\n",
      "tokenizer.json: 100%|██████████| 17.5M/17.5M [00:01<00:00, 12.4MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 555/555 [00:00<00:00, 1.06MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa-2.0` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 718/718 [00:00<00:00, 588kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 21.1kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.47MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.50MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 10.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `openai-community/gpt2-medium` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 702/702 [00:00<00:00, 307kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 376/376 [00:00<00:00, 293kB/s]\n",
      "spiece.model: 100%|██████████| 4.31M/4.31M [00:00<00:00, 12.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 65.0/65.0 [00:00<00:00, 54.6kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `google/mt5-base` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.42k/1.42k [00:00<00:00, 1.06MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.92k/1.92k [00:00<00:00, 1.32MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 6.91MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 12.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 1.79k/1.79k [00:00<00:00, 1.16MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n",
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 494/500 [1:04:08<00:48,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `vennify/t5-base-grammar-correction` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 747/747 [00:00<00:00, 261kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 223/223 [00:00<00:00, 58.4kB/s]\n",
      "tokenizer.json: 100%|██████████| 14.5M/14.5M [00:01<00:00, 12.8MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 85.0/85.0 [00:00<00:00, 56.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `bigscience/bloomz-7b1` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 661/661 [00:00<00:00, 433kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 444/444 [00:00<00:00, 376kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 6.05MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 303/303 [00:00<00:00, 200kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 594/594 [00:00<00:00, 187kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 700/700 [00:00<00:00, 203kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.4MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 12.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 209kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `huggyllama/llama-7b` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 583/583 [00:00<00:00, 186kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 595kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 12.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 8.24MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 232kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/Wizard-Vicuna-13B-Uncensored` from `transformers`...\n",
      "Loading pretrained config for `cognitivecomputations/Wizard-Vicuna-13B-Uncensored` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 582/582 [00:00<00:00, 271kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 564kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 13.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 11.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 225kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `cognitivecomputations/Wizard-Vicuna-7B-Uncensored` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.07k/1.07k [00:00<00:00, 336kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 727/727 [00:00<00:00, 576kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 10.9MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 10.6MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 14.6kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 347kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ` from `transformers`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:04:33<00:00,  7.75s/it]\n"
     ]
    }
   ],
   "source": [
    "response = get_tgi_models_and_parse(limit=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 97.23ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "160148"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "\n",
    "ds = Dataset.from_list(response)\n",
    "\n",
    "ds.to_csv(\"tgi_models.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_models_and_parse(limit=25,type=\"likes30d\",filter=\"sentence-transformers\"):\n",
    "  url=f\"https://huggingface.co/api/models?sort={type}&direction=-1&filter={filter}&limit={limit}\"\n",
    "  response = r.get(url, headers=headers).json()\n",
    "  # supported TEI architectures\n",
    "  architectures = [\"bert\", \"roberta\", \"xlm-roberta\",\"nomic_bert\"]\n",
    "  # map, filter list to remove gguf \n",
    "  filtered_models=[]\n",
    "  for model in response:\n",
    "    try:\n",
    "      # backlist models which leads to crashes on my mac\n",
    "      if model[\"id\"] in []:\n",
    "        continue\n",
    "      # filter TEI supported architectures\n",
    "      if not any(architecture in model[\"tags\"] for architecture in architectures):\n",
    "        continue\n",
    "      \n",
    "      # get license \n",
    "      license_value = next((tag.split(':', 1)[1] for tag in model[\"tags\"] if tag.startswith('license:')), \"N/A\")\n",
    "      \n",
    "      # check for gate\n",
    "      gated = check_for_gate(model[\"id\"])    \n",
    "                  \n",
    "      # model size   \n",
    "      filtered_models.append({\n",
    "        \"model_id\": model[\"id\"],\n",
    "        \"url\": f\"https://huggingface.co/{model['id']}\",\n",
    "        \"cotaniner\": \"PyTorch TEI CPU/GPU\",\n",
    "        \"license\": license_value,\n",
    "        \"gated\": gated,\n",
    "        \"private\": model[\"private\"],\n",
    "        \"likes\": model[\"likes\"],\n",
    "        \"likes30d\": model[\"likes30d\"],\n",
    "        \"downloads\": model[\"downloads\"],\n",
    "      })\n",
    "    except Exception as e:      \n",
    "      print(f\"Error parsing model {model['id']}\")\n",
    "      continue\n",
    "  return filtered_models  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = get_emb_models_and_parse(limit=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 480.17ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5781"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "\n",
    "emb_ds = Dataset.from_list(embedding)\n",
    "\n",
    "emb_ds.to_csv(\"emb_models.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
